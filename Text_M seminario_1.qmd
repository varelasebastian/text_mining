---
title: "Seminario Mineria de texto y modelos de lenguaje 2025"
toc: true
3toc-depth: 3
toc-title: "Tabla de contenidos"
author:
  - name: "Sebastián Varela"
    affiliation: "IdIHCs / Dpto de Sociología FaHCE UNLP"    
  - name: "Claudia González"
    affiliation: "IdIHCs / Dpto de Bibliotecología FaHCE UNLP" 
format: 
  pdf: 
    geometry: 
      - top=2cm
      - bottom=2cm
      - left=2cm
      - right=2cm
---

# **PARTE I:** MINERIA DE TEXTOS *(introductorio)*
**por Claudia M. González**

# Objetivo del módulo

En esta primera parte del seminario se revisarán algunas de las técnicas más tradicionales de la minería textual, así como los tratamiento previos más habituales a los que se someten los corpus textuales antes de ser analizados.

Se parte por conformar un conjunto de datos que se utilizará como caso de análisis en todo el módulo. De manera independiente a que los resultados obtenidos sean interesantes o no, su uso obedece a cuestiones meramente didácticas.

Durante los ejercicios se trabajará con diversos paquetes, los cuales deberán estar instalados previamente haciendo uso de los cuadros de diálogo de R Studio (opción Tools). A continuación se brinda una muy breve descripción de cada uno.

**Tidyverse** es una colección de paquetes de R para la importación, transformación, manipulación y visualización de datos ordenados. 

**ggplot2** es el paquete más importante de R para resolver visualizaciones de datos.

**OpenalexR** es un paquete que permite recuperar información de la base de datos Open Alex y deja los datos preparados para ser manipulados en R base o por otros paquetes.

**DataExplorer** permite obtener salidas visuales útiles al realizar una primera exploración del conjunto de datos.

**Textclean** permite detectar subcadenas cuya presencia no ayuda al momento del análisis. Puede borrarlas o normalizarlas.

**Tidytext** entre sus principales funciones se encuentran las dedicadas a diferentes niveles de tokenización.

**Stopword** permite aplicar la limpieza de palabras no significativas en diferentes idiomas.

**SnowballC** se utiliza para hacer stemming o llevar cada palabra a su raíz lingüistica 

**wordcloud2**, **webshot2** y **htmlwidgets** el primero permite generar nubes de palabras. Los otros dos paquetes son necesarios para que las nubes se puedan imprimir en el PDF de salida de un quart document.

**quanteda**, **quanteda.textstats** y **quanteda.textplots** es un paquete especializado en data_minning

**Matrix** se utiliza para manipular matrices.

**Widyr** si bien es parte del ecosistema tidy, necesita su propia instalación. Sirve para calcular frecuencias conjuntas. 

**tidygraph**, **igraph** y **ggraph** se utilizan para graficar.

```{r}
library(tidyverse)
library(ggplot2)
library(openalexR)
library(DataExplorer)
library(textclean)
library(tidytext)
library(stopwords)
library(SnowballC)
library(wordcloud2)
library(webshot2)
library(htmlwidgets)
library(quanteda) 
library(quanteda.textstats)
library(quanteda.textplots)
library(Matrix)
library(widyr)
library(tidygraph)
library(igraph)
library(ggraph)

```


# Selección de los datos a trabajar

## Seleccionar la fuente de datos

En esta primera parte del seminario realizaremos las prácticas sobre un conjunto de datos textuales que emanan de **registros bibliográficos referenciales sobre sociología argentina**. El concepto de *referencial* es importante para entender que no estamos hablando de textos completos de artículos o libros, sino solo de sus datos bibliográficos: titulo, resumen, keywords, etc. Obedece a que resulta, en terminos prácticos, trabajar con un corpus textual acotado en cuanto a la longitud del texto de cada item, de esa manera se pueden realizar los checkeos de las acciones con mayor facilidad.

La elección de la temática de la producción argentina sobre temas sociológicos es totalmente arbitraria. Podríamos haber aplicado cualquier otro recorte que desearamos. Lo que sí resulta determinante, es que elegimos **OpenAlex** como fuente donde realizar la búsqueda.

**OpenAlex** (https://openalex.org) es una base de datos bibliográfica abierta, de alcance global, de reciente gestación (2022), pero que ha tenido un crecimiento muy acelerado. Ofrece en la actualidad más de 250 millones de registros bibliográficos de producción científica de todo el mundo. Es multidiciplinar y pone su esfuerzo en incluir materiales en otros idiomas además del inglés. Tuvo su origen en Microsoft Academic Graph, producto que se discontinuó, y sirvió de base para iniciar este mega proyecto que cosecha más de 250 mil fuentes distintas. La base se puede consultar mediante una API REST pública, que es gratuita y no requiere autenticación. Sin embargo, tiene limitaciones de velocidad, con un límite de 100.000 llamadas por día y un máximo de 10 llamadas por segundo. Por fuera de eso, ofrece un servicio premium. En nuestro caso, interrogaremos la base de datos utilizando un paquete de R que fue especificamente diseñado para mediar en esta tarea, que se llama **openalexR**. Siemplemente nos exime del uso de la API de forma directa.

La estructura de información que maneja esta base de datos tiene cierta complejidad y no es objetivo de este seminario abordarla. Simplemente, nos interesa reforzar la misma idea que ya se transmitió en los seminarios anteriores: que un buen conocimiento de la estructura de información subyacente de cualquier fuente, así como saber cuál es la forma correcta de interrogarla, son aspectos imprescindibles a conocer y que esto antecede a cualquier extracción de datos a los fines de ejecutar análisis serios.

## Buscar lo que interesa

Al trabajar con una fuente multilingüe, una primera decisión es determinar los idiomas que nos interesan que estén representados en nuestro conjunto de producción científica. En este caso, utilizaremos como término de búsqueda las versiones correspondientes a los idiomas español, inglés, italiano, francés, portugués y alemán. Al incluir esos idiomas, consideramos que estamos recogiendo la mayoría de la producción de nuestros autores argentinos.

Otro aspecto importante es tener claro cuales son los campos de datos de la base de datos que se utilizan en la interrogación. Solo para dar ejemplos de variantes posibles, una cosa es buscar un *string* únicamente en los títulos, otro que aparezca en los resúmenes o en ambos. Recordemos que cada campo de una base de datos es una unidad lógica que puede ser explotada individualmente, o en conjunto con otras, si es que usamos operaciones lógicas Booleanas (AND, OR, NOT), por ejemplo. Sin embargo, estas cuestiones no siempre son tan transparentes en los productos y es necesario, como ya se dijo, conocer lo mejor que se pueda la fuente por las implicancias que tiene a los fines de la recuperación de información.

En nuestro caso utilizaremos una forma de búsqueda muy sencilla, en la que indicamos que queremos obtener los registros bibliográficos de trabajos que contienen en el resumen "Sociología" o sus variantes en los idiomas antes indicados: sociología (ES), sociology (EN), sociologie (FR), sociologia (IT y PT) y soziologie (DE). Claramente está es una expresión de búsqueda muy rudimentaria que está lejos de dar cuenta de la producción científica argentina sobre Sociología, pero solo la tomamos como un ejemplo funcional a nuestra causa de colecta de datos a trabajar.

El otro elemento que se incluye en la búsqueda, es la restricción de que los trabajos a recuperar deben tener al menos un autor que acompaña su firma con un lugar de trabajo en Argentina.

```{r warning=FALSE}
socio_ar <- oa_fetch(abstract.search = "sociología OR sociologia OR sociology OR sociologie OR soziologie",
                     authorships.institutions.country_code = "AR")
```

La función **oa_fetch** es propia del paquete **openalexR** y opera como si fuese una API de consulta a la base de datos, siguiendo los parámetros que se le indican como argumento.

El resultado que arroja es un objeto R de tipo dataframe tibble, que de ahora en más pasa a constituir nuestro dataset de trabajo. Al mirarlo, comprobamos que hemos descargado 1895 trabajos los cuales están descriptos por 44 variables. La cantidad de trabajos puede variar según el momento en el que se hace la búsqueda y descarga.

Para visualizar las 44 variables que hay en el dataset:

```{r}
names(socio_ar)
```

En nuestro ejercicio, vamos a trabajar con todas las observaciones, pero no con todas las variables. Nos quedaremos solo con alguna básicas: el identificador del trabajo, el título, el resumen, los tópicos y el idioma del documento.

Para seleccionar las variables que interesan:

```{r}
socio_ar_selec <- socio_ar %>% 
  select(id,title,abstract,topics,language)
```

## Análisis de valores perdidos

Una primera exploración sobre los datos que siempre es aconsejable hacer es el análisis de valores perdidos. Para visualir la proporción de faltantes de manera gráfica, se necesita el paquete DataExplorer. En este caso ya está instalado y corriendo.

```{r, fig.width=5}
plot_missing(socio_ar_selec) 
```

Para saber que trabajos son a los que le falta un dato, que de acuerdo a como lo muestra el gráfico anterior corresponde al campo título e idioma:

```{r}
socio_ar_selec %>%
  filter(if_any(everything(), is.na))
```

Si se toma los últimos dígitos del ID de los trabajos, se puede buscar por cuadro de diálogo en el dataset para verificar que efectivamente son registros sin títulos o sin idioma.

# Pre-procesamiento de los textos

Una vez que contamos con el corpus textual a trabajar, comenzamos con algunos procesamientos que es necesario hacer antes de tratar de obtener unos primeros resultados. Para ello se realizan algunas tareas que permiten quitarle "ruido" al texto. Estas tareas consiste en la limpieza de símbolos,
 la aplicación de ***stopword*** y la aplicación de algoritmos de  ***stemming***. También se transforma el texto aplicando ***tokenization*** y ***N-grams***, si se desea.

## Limpieza del texto

Principalmente se realiza una limpieza de los residuos que puedan quedar de HTML y de tildes. Cualquier trabajo más fino requiere, por lo general conocer el uso de expresiones regulares.

Las expresiones regulares 

```{r}
socio_ar_selec_limp <- socio_ar_selec
socio_ar_selec_limp$title <- socio_ar_selec$title %>%
  replace_html() %>%
  replace_contraction() %>%
  replace_non_ascii () %>%
  replace_url () %>%
  replace_email ()

  
socio_ar_selec_limp$abstract <- socio_ar_selec$abstract %>%
  replace_html() %>%
  replace_contraction() %>%
  replace_non_ascii () %>%
  replace_url () %>%
  replace_email ()
```

Si miramos los resultados, vemos que la puntuación general permanece, así como las mayúsculas (comprobar buscando GERMANI). Para eliminar:

```{r}
socio_ar_selec_limp <- socio_ar_selec_limp %>%
  mutate(
    title = str_remove_all (title,'[[:punct:]]'),
    abstract = str_remove_all (abstract,'[[:punct:]]')
  )

socio_ar_selec_limp$title <- tolower(socio_ar_selec_limp$title)
socio_ar_selec_limp$abstract <- tolower(socio_ar_selec_limp$abstract)
```

## Tokenization

Esto implica separar el texto en partes más pequeñas. Lo usual es hacerlo por palabras. Produce lo que se denomina una *bag of words* (BOW) o bolsa de palabras, que significa simplemente que se rompen las relaciones gramaticales que normalmente existen en el texto que integran. De las 6 variables que tiene nuestro corpus, solo 2 contienen textos ricos: los títulos y los resúmenes. Los tópicos, por el contrario son en muchos casos términos compuestos que no sería adecuado separar. Mientras título y resumen obedecen al lenguaje natural, los tópicos, por el contrario, obedecen al lenguaje controlado y eso tiene efectos diferentes como ya veremos. Por ahora excluímos los tópicos de la tokenización. Sin embargo, vamos a mantener la información de ID del documento y del idioma.

Para tokenizar por palabras:

```{r}
palab_tit <- socio_ar_selec[, c("id","title","language")] %>%
  unnest_tokens(palabra,title)

palab_res <- socio_ar_selec[, c("id","abstract","language")] %>%
  unnest_tokens(palabra,abstract)
```

Se generan dos objetos de datos para que luego se pueda comparar resultados. Uno corresponde a los títulos de los trabajos tokenizados, y el otro a los títulos + los resúmenes tokenizados. Estos listados de palabras son dataframes tibble que contienen 3 variables: el id del documento, la palabra y el idioma. Mantener la información del item u observación que aporta cada una de las palabras es lo que hace a la idea de **corpus**. Un **corpus** hace referencia a un conjunto de textos que durante ciertos tratamientos no deben perder su identidad, es decir que se pueden tratar como pequeñas BOW documento por documento. Al sumar título y resumen, consideramos que la repetición de términos en uno y otro campo servirá para reforzar de lo que trata el documento.

Para unir los títulos con los resúmenes:

```{r}
palab_tit_res <- bind_rows(palab_tit, palab_res)
```

## Stopword

Aplicar stopword es eliminar del listado general de palabras (o *bag of words*) aquellas que no tienen demasiada carga de sentido, tales como artículos, preposiciones, pronombres, etc. Para poder hacer esa tarea, se necesita contar previamente con listados de palabras a eliminar en cada idioma.

En este caso nos interesan los idiomas que están representados en nuestro corpus:

```{r}
stopwords_es <- tibble(palabra = stopwords::stopwords("es"),language = "es")
stopwords_en <- tibble(palabra = stopwords::stopwords("en"),language = "en")
stopwords_it <- tibble(palabra = stopwords::stopwords("it"),language = "it")
stopwords_fr <- tibble(palabra = stopwords::stopwords("fr"),language = "fr")
stopwords_pt <- tibble(palabra = stopwords::stopwords("pt"),language = "pt")
stopwords_de <- tibble(palabra = stopwords::stopwords("de"),language = "de")
```

Una vez que tenemos los listados, podemos proceder a la eliminación, la que simplemente se realiza por comparación de string de caracteres. Lo haremos sobre el conjunto de datos que contiene las palabras de los títulos, por un lado, y en el conjunto que tiene las palabras de los títulos + resúmenes. 

Para aplicar stopword usamos el anti_join, que sirve para filtrar las filas de un dataframe en función de que no estén presentes en otro. Lo hacemos por idiomas: 

```{r}
palab_tit_limp <- palab_tit %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra") 
  
palab_tit_res_limp <- palab_tit_res %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra") 
```

* [caso a revisar 502678 título en español, abstract en inglés, language en o 610174 título y resumen en inglés, language es]

## N-grams (ventanas de tokenización)

Como hemos visto hasta aquí, trabajar con **bag of words** tiene el inconveniente de la desvinculación de las palabras que componen términos copuestos. Un tipo de procesamiento que se puede aplicar para tratar de encontrar una mejora en este sentido, es con la detección de bigramas, es decir con ventanas de dos palabras. También podrían ser tri-gramas, cuatri-gramas, y así.

a - Obtenemos los bigramas de los títulos ordenados por freq

```{r}
bigram_tit_id <- palab_tit_limp %>%
  select (id,palabra) %>%
  group_by(id) %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  count(id,bigram, sort = TRUE)
bigram_tit_id
```

b - Bigramas de los títulos + resumen ordenados por freq

```{r}
bigram_tit_res_id <- palab_tit_res_limp %>%
  select (id,palabra) %>%
  group_by(id) %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  count(bigram, sort = TRUE)
```

## Stemming

El *stemming* es el proceso de reducción de las variantes de las palabras hasta llegar a obtener el lema (entrada de diccionario). Los algoritmos que existen basan su procedimiento en la eliminación de prefijos o sufijo haciendo chequeos contra listas de comprobación y aplican algunas reglas propias por idioma. Se debe tener en cuenta que esto muchas veces hace que lo que se obtiene finalmente no sean palabras reales. Si bien el fin último es la lematización -llegar a entradas de diccionario-, no siempre es posible, y como es de esperar, los algoritmos trabajan mejor en el idioma inglés. El algoritmo más usado es el de Porter (1980), pero también son conocidos los llamados **S**, **Lovins**, **Lancaster** y **snowball**.

Aplicamos el stemming **snowball** de manera separada por idiomas:

```{r}
stem_tit_es <- palab_tit_limp %>%
  filter(language == "es") %>%
  mutate(stem = wordStem(palabra,language = "spanish"))

stem_tit_res_es <- palab_tit_res_limp %>%
  filter(language == "es") %>%
  mutate(stem = wordStem(palabra,language = "spanish"))

stem_tit_es  
```

```{r}
stem_tit_fr <- palab_tit_limp %>%
  filter(language == "fr") %>%
  mutate(stem = wordStem(palabra,language = "french"))

stem_tit_res_fr <- palab_tit_res_limp %>%
  filter(language == "fr") %>%
  mutate(stem = wordStem(palabra,language = "french"))

stem_tit_fr  
```

```{r}
stem_tit_it <- palab_tit_limp %>%
  filter(language == "it") %>%
  mutate(stem = wordStem(palabra,language = "italian"))

stem_tit_res_it <- palab_tit_res_limp %>%
  filter(language == "it") %>%
  mutate(stem = wordStem(palabra,language = "italian"))
```

```{r}
stem_tit_en <- palab_tit_limp %>%
  filter(language == "en") %>%
  mutate(stem = wordStem(palabra,language = "english"))

stem_tit_res_en <- palab_tit_res_limp %>%
  filter(language == "en") %>%
  mutate(stem = wordStem(palabra,language = "english"))
```

```{r}
stem_tit_pt <- palab_tit_limp %>%
  filter(language == "pt") %>%
  mutate(stem = wordStem(palabra,language = "portuguese"))

stem_tit_res_pt <- palab_tit_res_limp %>%
  filter(language == "pt") %>%
  mutate(stem = wordStem(palabra,language = "portuguese"))
```

Para el idioma alemán no lo hacemos, porque hemos verificado que en este corpus no existen trabajos en alemán.

Las volvemos a juntar en un solo dataframe

```{r}
palab_tit_stem <- bind_rows(stem_tit_es,stem_tit_en,stem_tit_fr,stem_tit_it,stem_tit_pt)
  
palab_tit_res_stem <- bind_rows(stem_tit_res_es,stem_tit_res_en,stem_tit_res_fr,stem_tit_res_it,stem_tit_res_pt)
```

# Técnicas

## Frecuencias y nubes de palabras

### Conteo de frecuencias
El conteo de frecuencias de palabras es el análisis más básico. La generación de NUBE DE PALABRAS que representa al corpus, es la expresión gráfica asociada.

Con los dos pasos anteriores -el de la tokenización y el de la eliminación de palabras no significativas-, hemos terminado de preparar la versión más simple de nuestro datos. Lo más simple es realizar un conteo de frecuencia de palabras como para ver si esa información nos dice algo sobre los perfiles temáticos de la producción científica argentina sobre Sociología. Vamos a incorporar en este análisis tambien la variable topics. Si bien provee menos cantidad de texto, como no la hemos tokenizado, mantiene la vinculación entre términos compuestos. La cantidad de observaciones que obtenemos nos está indicando la cantidad de palabras diferentes que contiene la variable(s) analizadas en la totalidad del corpus.

Para calcular la frecuencia lo hacemos sobre las palabras con stopword:

```{r}
tit_freq <- palab_tit_limp %>%
  count(palabra, sort = TRUE)
tit_freq
```

```{r}
tit_res_freq <- palab_tit_res_limp %>%
  count(palabra, sort = TRUE)
tit_res_freq
```

También vamos a aplicar este tipo de análisis sobre la variable topics(Tópicos). Si bien provee menos cantidad de texto que las variables título y resúmen, como no la hemos tokenizado, mantiene aún el vínculo entre los términos compuestos. Esta variable es un dataframe tibble en sí misma, es decir que estamos frente a un anidamiento de dataframes. Cuando la exploramos, vemos que los términos aparecen calificados con un "tipo". Los tipos se adjudican por especificidad creciente: dominio, campo, subcampo y tópico. Aquí nos interesa analisar los más específicos, es decir, aquellos que están calificados con el 4to nivel (tópic). Para ello generamos el objeto tópicos. Vemos que necesitamos escribir un poco más de código por esa condición de no ser una columna normal de un dataframe, sino de ser una columna que contiene a su vez un dataframe adentro (dataframe anidado):

**unnest** permite desplegar el dataframe dentro del dataframe general **mutate** sirve para renombrar las columnas **filter** sirve para filtrar por la categoría de tópico. En este caso usaremos "topic" **select** para seleccionar las columnas que me interesan

```{r}
topicos <- socio_ar_selec %>%
  unnest_longer (topics) %>%
  mutate(topics_id = topics$id, topics_type = topics$type,
         topics_name = topics$display_name) %>%
  filter(topics_type == "topic") %>%
  select(topics_id,topics_name)
```

Calculamos las frecuencias de los tópicos

```{r}
top_freq <- topicos %>%
  count(topics_name, sort = TRUE)
top_freq
```

### Obtener resumen 

En este punto, parece apropiado hacer un resumen que nos permita ver el tamaño de los distintos conjuntos de datos que vamos obteniendo:

```{r}
info <- c('Cant. de trabajos','Cant. palab títulos',
          'Cant. palab títulos stopword',
          'Cant. palab títulos únicas',
          'Cant. palab títulos+resumen',
          'Cant. palab títulos+resumen stopword',
          'Cant. palab títulos+resumen únicas',
          'Cant. tópicos','Cant. tópico únicos',
          'Cant. bigramas títulos con ID',
          'Cant. bigramas títulos+resumen con ID')
cant <- c(c(nrow(socio_ar),nrow(palab_tit),nrow(palab_tit_limp),
            nrow(tit_freq),nrow(palab_tit_res),
            nrow(palab_tit_res_limp),nrow(tit_res_freq),
            nrow(topicos),nrow(top_freq),nrow(bigram_tit_id),
            nrow(bigram_tit_res_id)))

resumen <- tibble(col1 = info,col2 = cant)
resumen
```

### Nubes de palabras

Antes de la generación de las nubes de palabras, aplicaremos, si lo creemos necesario, algún criterio de poda por frecuencia. En este caso, generaremos 3 nubes de palabras para poder comparar variantes. Generaremos la de los títulos por palabras individuales sin poda de frecuencias. La de los títulos por bigramas y la de tópicos las haremos con frecuencias mayores a 1.

```{r}
tit_freq_2 <- tit_freq %>%
  filter(n > 1) 
```

```{r}
bigram_tit_f <- bigram_tit_id %>%
  ungroup() %>%
  select(bigram,n) %>%
  filter(n > 1) 
```

```{r}
top_f <- top_freq %>%
  filter(n > 1) 
```

Nube de palabras de los títulos
```{r}
nube_tit <- wordcloud2(tit_freq, size = 0.8)
```

Esta parte de código que NO es importante, solo es necesaria para el renderizado y muestra de la nube en el PDF final.

```{r}
options(webshot.quiet = TRUE)
invisible({
html_temp <- tempfile(fileext = ".html")
  img_output <- "nube_tit.png"
  suppressMessages(saveWidget(nube_tit,html_temp,selfcontained = FALSE))
  capture.output(
    webshot2::webshot(html_temp, img_output, delay = 7, vwidth = 1000, vheight = 800,zoom = 2),
    file = nullfile())
  unlink(html_temp, force = TRUE)
  })
  
knitr::include_graphics(img_output)
```

Con un código similar, se pueden crear, por ejemplo, la de bigramas de títulos,

```{r}
nube_bigram <- wordcloud2(bigram_tit_f, size = 0.2)
```

Código renderizado PDF
```{r}
invisible({
html_temp <- tempfile(fileext = ".html")
  img_output <- "nube_bigram.png"
  suppressMessages(saveWidget(nube_bigram,html_temp,selfcontained = FALSE))
  capture.output(
    webshot2::webshot(html_temp, img_output, delay = 7, vwidth = 1000, vheight = 800,zoom = 2),
    file = nullfile())
  unlink(html_temp, force = TRUE)
})
  
knitr::include_graphics(img_output)
```

o la de tópicos
```{r}
nube_top <- wordcloud2(top_f, size = 0.6) 
```

Código renderizado PDF
```{r}
invisible({
html_temp <- tempfile(fileext = ".html")
  img_output <- "nube_top.png"
  suppressMessages(saveWidget(nube_top,html_temp,selfcontained = FALSE))
  capture.output(
    webshot2::webshot(html_temp, img_output, delay = 7, vwidth = 1000, vheight = 800,zoom = 2),
    file = nullfile())
  unlink(html_temp, force = TRUE)
})

knitr::include_graphics(img_output)
```

## TF-IDF y matrices documento/término (DTM)

Ya se mostró que las nubes de palabras simplemente están mostrando las frecuencias contabilizadas a nivel del corpus completo, visto como una BOW. 

Ahora bien, siguiendo los antecedentes que propuso **Spark-Jones**, que a los fines de la recuperación de información, el mejor poder de discriminición se encuentra en las palabras que tienen una alta frecuencia intra-documental, pero también una baja frecuencia a nivel de la colección de documentos (corpus).

TF-IDF = TF * IDF

TF(t,d) = (Número de veces que aparece el término t en el documento d) / (Número total de términos en el documento d)

IDF(t) = log 10 ((Número total de documentos) / (Número de documentos que contienen el término t))

TF-IDF se ha utilizado ampliamente como una formula de ponderación de los términos en un corpus. 

La forma para realizar esos cálculos de frecuencias es organizar el corpus de manera bidimensional, donde las filas pueden ser todos los documentos y las columnas los términos de la totalidad del corpus (o a la inversa). Cada elemento individual, es decir la intersección de un término en un documento dado, puede tomar un valor simple como un 0 si el término está ausente o un 1 si está presente. Esto se conoce como **frecuencia binaria** aunque en rigor no es una frecuencia. Si en lugar de eso ponemos los valores de **frecuencias absolutas**, resultarán siempre favorecidos los documentos más largos (a mayor vocabulario má sposibilidad de repetición de términos). Otra forma para reducir la variabilidad que tenga el corpus en cuanto a la longitud de los documentos es utilizar lo que se denomina **frecuencia logarítmica** log 10 (1+frecuencia absoluta). La **frecuencia relativa** al tamaño del documento suele ser muy utilizada.Un poco más sofisticado sería, entonces, poner el valor resultante del cálculo TF-IDF. 

Un uso frecuente que se le da a estas matrices originarias de la RI (recuperación de información) es la TM (minería textual) con fines de determinar de que tratan los textos.

En esta práctica, se generan las matrices TF-IDF para el corpus de títulos:

```{r warning: FALSE}
#Uso paquete quanteda

corpus_tit <- corpus(socio_ar_selec$title)

head(corpus_tit)
```

```{r}
tokens_tit <- tokens(corpus_tit,remove_punct = TRUE) %>%
  tokens_remove(stopwords("es")) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_remove(stopwords("fr")) %>%
  tokens_remove(stopwords("it")) %>%
  tokens_remove(stopwords("pt"))

head(tokens_tit)
```

```{r}
dfm_tit <- dfm(tokens_tit)

head(dfm_tit)
```

```{r}
dfm_tfidf_tit <- dfm_tfidf(dfm_tit)

head(dfm_tfidf_tit)
```


Observamos que el resultado es una matriz gigante con más de 11 millones de elementos. Es rectangular de 1895 (documentos) x 5962 (términos o palabras). Las primeras 100 filas de la matriz muestran:

```{r}
top_terms <- textstat_frequency(dfm_tfidf_tit, n= 100, force =TRUE) %>%
  as_tibble()

top_terms
```

Esto se interpreta como:
col1 = termino,
col2 = suma global de los valores TF-IDF de ese término en todos los documentos,
col3 = rango = ranking (1 el + frec),
col4 = número de documentos donde aparece el término.

Si queremos podríamos eliminar los 3 primeros términos de la ecuación diciendo que elimine del cálculo a los términos que estén en más de 150 documentos.

```{r, warning=FALSE}
dfm_tit_2 <- dfm_trim (
  dfm_tit,
  max_docfreq = 150)

dfm_tfidf_tit_2 <- dfm_tfidf(dfm_tit_2)
```

Técnicas posibles para visualizar estas matrices con algo de sentido: nuevamente apelamos a las nubes de términos.

```{r, warning=FALSE}
quanteda.textplots::textplot_wordcloud(
  dfm_tfidf_tit_2,
  max_word = 60,
  color = c("blue", "red", "green"),
  rotation = 0.3,
  min_size = 0.5, 
  max_size = 4)
```

Se debe tener en cuenta que estas nubes no muestras nunca ni co-ocurrencia, ni correlación, ni distancias semánticas. Son solo frecuencias TF-IDF. Las palabras se disponen siguiendo un algoritmo de ubicación espacial óptima que gestiona los diferentes tamaños.

## Medidas de asociación y la detección de colocaciones

El objetivo es identificar las combinaciones de palabras más significativas. Para eso se detectan los bigramas o trigramas que aparecen juntos más frecuentemente de lo esperado por azar. 

Para determinar esto, se utiliza alguna métrica estadística del tipo Lambda (o log-lambda), PMI (Pointwise Mutual Information) o Chi2 (CHI cuadrado), entre otras. En este caso utilizaremos el paquete **Quantera** que tiene implementada una función denominada **textstats_collocations**. Esta función utilizando el estadístico Lambda. 

Se considera que Lambda da un buen balance, en definitiva, entre el *recall*, esa capacidad del sistema de recuperar todo lo relevante, y la precisión, es decir, de ser muy certero con lo que recupera. Hay que tener en cuenta que estas medidas pueden comportarse de distinta manera según el tamaño de los corpus. Para corpus muy grandes se recomienda CHI2. Quanteda considera que Lambda es apropiada para corpus chicos-medianos or su robustez y tratamiento hacia los datos raros. Evalúa cuán inesperadamente frecuente es la co-ocurrencia de dos o más palabras, comparando:

**- Frecuencia observada** (cuántas veces aparecen juntas en el corpus).
**- Frecuencia esperada** (cuántas veces se esperaría que aparecieran juntas si su distribución fuera aleatoria).

Valores cercanos a 3.84, que corresponde a un p < 0.05 ya se considera una colocación significativa, que las palabras tienden a aparecer juntas más de lo esperado por azar. Valores más altos (por ejemolo 12.3), serán muy significativas. Valores ~0, muy poco significativas.

En nuestro ejemplo lo aplicaremos sobre los títulos + resúmenes. Primero, entonces, generamos el corpus: 

```{r}
corpus_tit_res <- corpus(paste(socio_ar_selec_limp$title, socio_ar_selec_limp$abstract, sep = " ")) 

head(corpus_tit_res)
```

Luego se tokeniza:

```{r}
corpus_tit_res_tok <- tokens(corpus_tit_res) 

head(corpus_tit_res_tok)
```

Se aplica stopword:
```{r}
corpus_tit_res_tok <- tokens(corpus_tit_res) %>%
  tokens_remove(stopwords("es")) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_remove(stopwords("fr")) %>%
  tokens_remove(stopwords("it")) %>%
  tokens_remove(stopwords("pt")) 
 
head(corpus_tit_res_tok)
```

```{r}
collocations_tit_res <- textstat_collocations(
  corpus_tit_res_tok,
  size = 2,          # Bigramas
  min_count = 2,     # Mínimo 2 apariciones
  method = "lambda"  # Métrica estadística
)

collocations_tit_res <- collocations_tit_res[order(-collocations_tit_res$lambda), ]

head(collocations_tit_res)
```

En la salida observamos que nos da también el z-score como medida complementaria. El z-score (o puntuación z) es una medida que indica cuántas desviaciones estándar se aleja la frecuencia observada de la frecuencia esperada bajo independencia.

Un z > 3 indica que la colocación es estadísticamente significativa (p < 0.001). z ≈ 0, no hay asociación significativa y z < 0, las palabras co-ocurren menos de lo esperado (repulsión).

## Redes de co-palabras

Si bien el análisis anterior nos brinda información bastante sustancial, si queremos verla en una síntesis gráfica, tendremos que apelar a las llamadas redes de co-ocurrencia de palabras. 

Estas redes tienen como andamiaje conceptual el Análisis de Redes Sociales (ARS), técnica que explicita, fundamentalmente, la intensidad de las relaciones utilizando la teoria de grafos.

Los elementos principales son: 
1- Nodos (vértices): Representan los actores de la red. En nuestro caso serán las palabras o **collocations**.
2- Enlaces (aristas): Son las conexiones entre nodos. En nuestro caso serán las ocurrencias conjuntas.
3- Redes: Son los grafos resultantes que modelan relaciones de co-ocurrencia, que en nuestro caso serán no dirigidas, y podrían ser ponderadas/no ponderadas.

De las redes se pueden obtener algunas métricas. La smás comunes son:

Centralidad: Identifica nodos influyentes (grado, intermediación, cercanía).
Densidad: Mide el nivel de conexión en la red.
Comunidades: Grupos de nodos densamente conectados (ej: algoritmos como Louvain).

Para comenzar utilizaremos del paquete Quanteda la función **tokens-compound** que sirve para detectar tokens compuestos. Nosotros pediremos que esa detección la haga basándose en las **collocations** que obtuvimos en el paso anterior:


```{r}
corpus_tok_coll <- tokens_compound(corpus_tit_res_tok, collocations_tit_res)

head(corpus_tok_coll)
```

Una vez que tenemos el corpus tokenizado de acuerdo al patrón de las **collocations**, le vamos a aplicar stemming

```{r}
corpus_tok_coll_stem <- tokens_wordstem(corpus_tok_coll, 
             language = "spanish") 

corpus_tok_coll_stem <- tokens_wordstem(corpus_tok_coll_stem, 
             language = "english") 

corpus_tok_coll_stem <- tokens_wordstem(corpus_tok_coll_stem, 
             language = "french") 

head (corpus_tok_coll_stem)
```

lo convertimos en una matriz término/documento. Proponemos un umbral de 5 frecuencias como umbral mínimo de corte: 

```{r}
minimumFrequency <- 10
```

La matriz que generamos será de presencias/ausencias, es decir, no ponderada:

```{r}
dfm_dtm_bin <- corpus_tok_coll_stem %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = 1000) %>%   dfm_weight("boolean")

head(dfm_dtm_bin)
```

Estamos frente a una matriz rectangular de la que terminaremos derivando la matriz simétrica de co-ocurrencias. Esto se logra haciendo una multiplicación de matrices. La matriz original nDocs x nTerms se multiplica por su transpuesta nTerms x nDocs. Esto genera la matriz término/término de co-ocurrencias. Los valores pasan a ser la frecuencias de ocurrencias conjuntas, y en la diagonal queda registrado el valor de la frecuencia del término. Nos queda una mátriz simétrica ponderada por frecuencia.

```{r}
dfm_ttm <- t(dfm_dtm_bin) %*% dfm_dtm_bin

head(dfm_ttm)
```

Tratar de graficar estas matrices tan grandes, se hace muy bien con aplicaciones externas a R, como VOSViewer, por ejemplo. Los paquetes de R no resuelven muy bien esto. Lo que sí puede ser útil realizar, son vistas parciales, por ejemplo, tomar un término particular y analizar su contexto.  

Para nuestro ejemplo utilizaremos el término *pobreza* y usaremos una propuesta de análisis de los autores modelización desarrollada por el modelo de medición propuesto por Niekler y Wiedermann que adaptamos a nuestro caso.

La propuesta consiste en graficar la red que muestra el término elegido puesto en relación con otros.

Como primer paso mostramos que resultados arroja la medición de 3 medidas de significancia del término elegido contra los 20 primeros términos en el ranking de cada una de las medidas: MT (*mutual information*), coeficiente de Dice y Log-Likelihood (Log de verosimilitud)

```{r}
coocTerm <- "urban"
k <- nrow(dfm_dtm_bin)
ki <- sum(dfm_dtm_bin[, coocTerm])
kj <- colSums(dfm_dtm_bin)
names(kj) <- colnames(dfm_dtm_bin)
kij <- dfm_ttm[coocTerm, ]
```

MT - Mutual Information significancia
```{r}
mutualInformation_Sig <- log(k * kij / (ki * kj))
mutualInformation_Sig <- mutualInformation_Sig[order(mutualInformation_Sig, decreasing = TRUE)]
```

Dice
```{r}
dice_sig <- 2 * kij / (ki + kj)
dice_sig <- dice_sig[order(dice_sig, decreasing=TRUE)]
```

Log Likelihood
```{r}
log_like_sig <- 2 * ((k * log(k)) - (ki * log(ki)) - (kj * log(kj)) + (kij * log(kij)) 
               + (k - ki - kj + kij) * log(k - ki - kj + kij) 
               + (ki - kij) * log(ki - kij) + (kj - kij) * log(kj - kij) 
               - (k - ki) * log(k - ki) - (k - kj) * log(k - kj))
log_like_sig <- log_like_sig[order(log_like_sig, decreasing=T)]
```

Se juntan los resultados en un solo dataframe para evaluar, en definitiva que medida aplicar:

```{r}
# Poner todos los valores de significancia en un dataframe para verlos
resultOverView <- data.frame(
  names(sort(kij, decreasing=T)[1:20]), sort(kij, decreasing=T)[1:20],
  names(mutualInformation_Sig[1:20]), mutualInformation_Sig[1:20], 
  names(dice_sig[1:20]), dice_sig[1:20], 
  names(log_like_sig[1:20]), log_like_sig[1:20],
  row.names = NULL)
colnames(resultOverView) <- c("Terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LogL-Terms", "LogL")
print(resultOverView)
```

******** FUNCIÓN

```{r}
calculateCoocStatistics <- function(coocTerm, binDTM, measure = "DICE") {
  
  # Ensure Matrix (SparseM} or matrix {base} format
  require(Matrix)
 
  # Ensure binary DTM
  if (any(binDTM > 1)) {
    binDTM[binDTM > 1] <- 1
  }
  
  # calculate cooccurrence counts
  coocCounts <- t(binDTM) %*% binDTM
  
  # retrieve numbers for statistic calculation
  k <- nrow(binDTM)
  ki <- sum(binDTM[, coocTerm])
  kj <- colSums(binDTM)
  names(kj) <- colnames(binDTM)
  kij <- coocCounts[coocTerm, ]
  
  # calculate statistics
  switch(measure, 
         DICE = {
           dicesig <- 2 * kij / (ki + kj)
           dicesig <- dicesig[order(dicesig, decreasing=TRUE)]
           sig <- dicesig
         },
         LOGLIK = {
           logsig <- 2 * ((k * log(k)) - (ki * log(ki)) - (kj * log(kj)) + (kij * log(kij)) 
                          + (k - ki - kj + kij) * log(k - ki - kj + kij) 
                          + (ki - kij) * log(ki - kij) + (kj - kij) * log(kj - kij) 
                          - (k - ki) * log(k - ki) - (k - kj) * log(k - kj))
           logsig <- logsig[order(logsig, decreasing=T)]
           sig <- logsig    
         },
         MI = {
           mutualInformationSig <- log(k * kij / (ki * kj))
           mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]
           sig <- mutualInformationSig    
         },
         {
           sig <- sort(kij, decreasing = TRUE)
         }
        )
  sig <- sig[-match(coocTerm, names(sig))]
  return(sig)
}
```

*************FIN FUNCIÓN

Aplicamos la función para calcular el estadístico 
```{r}
# Definición de un parámetro para la representación de las co-ocurrencias de un concepto
numberOfCoocs <- 20
# Determinación del término contra el cuál serán medidos los restantes.
coocTerm <- "urban"
```

```{r}
coocs <- calculateCoocStatistics(coocTerm, dfm_dtm_bin, measure="DICE")

print(coocs[1:numberOfCoocs])
```

Generamos un grafo con **Igraph**. 

En la última matriz de coocurrencia calculada tenemos, en definitiva,  tripletas que tienen 3 datos: el término objetivo, el término con el que co-ocurre y el valor de significancia de la asociación. Para graficarla, esos valores se denotan como: from (nodo origen), to (nodo destino) y sig (ponderación por significancia). 

El proceso de armado del dataframe que da origen al grafo se ejecuta en dos pasos:
1- se obtienen todos los términos que co-ocurren de manera significativa con el término elegido.
2- se obtienen todas las co-ocurrencias de los términos del paso 1.

Los resultados intermedios de cada término se almacenan como tripletas temporales llamadas tmpGraph. Con el comando rbind todos los tmpGraph se añaden al objeto de red completo almacenado en resultGraph.

Se parte por definir la estructura del dataframe final, aún vacío

```{r}
resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
```

Se genera el dataframe temporario que se ira llenando

```{r}
tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

#el data frame para para el grafo temporario se llena con el código que sigue: 

#calcular el número correcto de filas a llenar. Más arriba habiamos definido el valor de numberOfCoocs en 15
tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]
#col 1: el término elegido
tmpGraph[, 1] <- coocTerm
#col 2: el término de co-occurrencia
tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]
#col3: valores de significancia
tmpGraph[, 3] <- coocs[1:numberOfCoocs]

head(tmpGraph)
```

Se suma esa dataframe temporal al dataframe definitiva
```{r}
resultGraph <- rbind(resultGraph, tmpGraph)
```

```{r}
# Iteración sobre las co-ocurrencias más significativas (numberOfCoocs) del término que se busca
for (i in 1:numberOfCoocs){
  
  # Se llama a la función que calcula las co-occurrence para el término i y el resto de palabras con las que co-occurrences
  newCoocTerm <- names(coocs)[i]
  coocs2 <- calculateCoocStatistics(newCoocTerm, dfm_dtm_bin, measure="DICE")
  
  #muestra las co-occurrencias
  coocs2[1:10]
  
  # Se estructura el objeto grafo temporal
  tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]
  tmpGraph[, 1] <- newCoocTerm
  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]
  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]
  
  #Se suman los resultados del grafo temporario al grafo resultante
  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])
}

# Muestra algunos ejemplos del grafo resultante
resultGraph[sample(nrow(resultGraph), 6), ]
```

```{r}
coocs <- calculateCoocStatistics(coocTerm, dfm_dtm_bin, measure="DICE")
# Visualización de los principales términos numberOfCoocs
print(coocs[1:numberOfCoocs])
```

```{r}
require(igraph)

# seteo de semilla
set.seed(123)

# Crear un objeto como grafo no dirigido
graphNetwork <- graph.data.frame(resultGraph, directed = F)

# Identificación de todos los nodos con menos de 4 aristas
verticesToRemove <- V(graphNetwork)[degree(graphNetwork) < 4]
# Estas aristas son removidas del grafo
graphNetwork <- delete.vertices(graphNetwork, verticesToRemove) 

# Asignación del color a los nodos (término buscado azul, otros naranja)
V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, 'cornflowerblue', 'orange') 

# Seteo de del color de las aristas
E(graphNetwork)$color <- adjustcolor("DarkGray", alpha.f = .5)
# sescala de significancia entre 1 y 10 para el ancho de la arista
E(graphNetwork)$width <- scales::rescale(E(graphNetwork)$sig, to = c(1, 10))
# Seteo de aristas con grosor
E(graphNetwork)$curved <- 0.15 
# Tamaño de nodos por su grado en la red (escala entre 5 y 15)
V(graphNetwork)$size <- scales::rescale(log(degree(graphNetwork)), to = c(5, 15))

# Define el marco y el espaciado en el grafo
par(mai=c(0,0,1,0)) 

# Gráfico
plot(
  graphNetwork,             
  layout = layout.fruchterman.reingold, # Algoritmo de graficación 
  main = paste(coocTerm, ' Graph'),
  vertex.label.family = "sans",
  vertex.label.cex = 0.8,
  vertex.shape = "circle",
  vertex.label.dist = 0.5,          # Las distancias etiq respectos a nodos 
  vertex.frame.color = adjustcolor("darkgray", alpha.f = .5),
  vertex.label.color = 'black',     # Color del nombre de nodos
  vertex.label.font = 1,            # Fuente del nombre de nodos
  vertex.label = V(graphNetwork)$name,      # nombre de los nodos
  vertex.label.cex = 1 # tamaño de fuente del nombre de los nodos
)

```

# Bibliografía

Porter, M. F. 1980. “An Algorithm for Suffix Stripping.” Program 14 (3): 130–137. https://doi.org/10.1108/eb046814.


***************************************************************************************************************************************************************************
# PARTE II: Modelos de lenguaje *(introductorio)*

## Grandes modelos de lenguaje 

Large Language Models (LLMs) de inteligencia artificial.

En este sección se usarán los paquetes ollamar, mall y ellmer.

* **Ollamar** es un paquete de R que actúa como una API de la plataforma Ollama. 
* **Ellmer**  es un paquete de R se utiliza para chatbots  funciona tanto con los modelos locales de Ollama como con otras APIs en la web como las de OpenIA, Mistral, DeepSeek, Anthropic, etc.
* **Mall** es un paquete de R se utiliza para datasets, y funciona con los modelos locales de Ollama.

## Ollamar

1.  Instalar el paquete ollamar

```{r, message=FALSE, warning=FALSE}
library(ollamar) #instalar antes en el rígido
```

2.  Bajar e instalar en la pc el soft de Ollama:

https://ollama.com/


3.  Bajar a la pc local algunos modelos para usar luego:

Navegar un poco la web de ollama buscando modelos. Estos se bajan al disco rígido sólo una vez.

```{r}
ollamar::pull("llama3.2") 
```

```{r}
ollamar::pull("deepseek-r1:1.5b") # red neuronal con 1.5 mil millones de parámetros!
```

```{r}
ollamar::pull("deepseek-llm:7b") # 7 mil millones de parámetros (más pesada para la pc local)
```

```{r}
ollamar::pull("deepseek-r1:8b")
```

```{r}
ollamar::pull("llama3:8b")
```

```{r}
ollamar::pull("llama2:13b")
```

## Ejercicios de aplicación 

## 1. Comunicación con chatbots de IA

### Ellmer Package

Este paquete solicitudes a cualquier proveedor de LLMs mediante la API de Ollama. También funciona con las APIs en la nube (ejemplo OpenAI, Mistral ect)

Ellmer usa LLMs locales:

* Ventajas: gratuidad, privacidad

* Desventajas: los LLMs son un poco menos avanzados que los disponibles vía web apis.

Empezaremos a usar los modelos locales provistos por Ollama. Esto tiene ventajas y desventajas como se explicó en la presentación de la clase.

### Chat básico con Ellmer

A diferencia de Rapp no voy a usar OpenIA sino Ollama. No necesito
setear clave con environmental variables.

Dee esta manera usamos con código modelos de lenguaje de la misma manera que lo hacemos con la interface de ChatGPT o cualquier otra:

```{r}
library(ellmer)  # Instalar previamente
```

```{r}
# Crea la sesión de chat y especifica el modelo local a usar
chat <- chat_ollama(    
  model = "llama3.2"
)
```

Ejecuto una consulta al modelo (prompt del usuario)
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Ahora creamos de nuevo la sesión, pero vamos a especificar el **system prompt**, parámetro que define instrucciones para guiar el comportamiento del modelo durante la sesión de chat:

```{r}
chat <- chat_ollama(
  model = "llama3.2",
  system_prompt = '"Sos un bot de AI que siempre responde lo siguiente: "Aguante Independiente de Avellaneda, el orgullo nacional!"'
)
```

Vuelvo a ejecutar la consulta:
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Observese como la respuesta cambia según el system_prompt:

```{r}
formal <- chat_ollama(model = "deepseek-llm:7b",  #cambio el modelo
                      system_prompt = "Actúa como un profesor universitario explicando conceptos de forma clara y formal.")

casual <- chat_ollama(model = "deepseek-llm:7b", 
                      system_prompt = "Habla como un amigo explicando de forma relajada y con ejemplos simples.")
```

```{r}
formal$chat("Explica en 70 palabras que es el overfitting")
```

```{r}
casual$chat("Explica en 100 palabras que es el overfitting")
```
También podemos pasar cualquier objeto externo, como una entrevista y pedirle al modelo que realice tareas de cualquier tipo, tal como hacemos con la interface habitual de ChatGPT. Aquí por ejemplo la vamos a pedir que resuma una entrevista. Se importa un entrevista como un objeto string (cadena de caracteres) y se asignan tareas al bot: generación de texto (analizar y resumir).

```{r}
#Bajar el archivo a la carpeta del proyecto desde este link: https://drive.google.com/file/d/135TC7Wy48UpYp-5V7wPmLF7btUdLSD3A/view?usp=sharing

# Leer el archivo de texto directamente como una sola cadena
entrevista <- paste(readLines("entrevista_militante.txt", 
                            encoding = "UTF-8", warn = FALSE), 
                   collapse = "\n") # une todas las línes leídas por readLines()
```


```{r}
modelo <- chat_ollama(model = "llama3.2",
                        system_prompt = "eres un bot muy conciso")

# 2. Crear el prompt completo
pregunta <- "Resume la entrevista en 100 palabras"
prompt_completo <- paste(pregunta, entrevista)  # Combina instrucción + texto input

# 3. Obtener y mostrar respuesta
resultado <- modelo$chat(prompt_completo)
resultado
```
La ventaja de usar chatbots mediante código es que permite integrarlos en procesos de datos.

## 2. Extracción de información de formatos no estructurados (IA extractiva)

Ellmer package. Se importa ahora un paper acádemico en formato como un objeto string (cadena de caracteres) y se asignan tareas:comprensión y extracción de información. Esto ya es más exigente, no para el modelo local sino para nuestras viejas PCs.



















```{r}
socio_ar_selec
```

Achico el dataset para las pruebas:
```{r}
set.seed(1234)  # Semilla aleatoria

# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm <- socio_ar_selec %>%
  sample_n(10)
```

Desenlistar la columna "topics", que es de tipo **list**: una lista en R es una estructura de datos versátil que puede contener elementos de diferentes tipos, como números, cadenas de texto, vectores e incluso otras listas. Una variable común en R almacena un único valor de un tipo de dato específico, mientras que una lista puede contener múltiples elementos de diferentes tipos (se puede entender como una mamushka)

```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest(topics, names_sep = "_") 
```

names_sep = "_" evita duplicaciones en nombres de columnas.

Ahora en cambio lo pivoteo para evitar la duplicación:
```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest_wider(topics, names_sep = "_")
```

### The Mall package

```{r}
library(mall) #se activa el paquete en la sesión
```

Al invocar las funciones de este paquete nos preguntará antes de correrla, que modelo de los bajados al rígido queremos usar. Por otro lado, también podemos elegir de antemano uno en particular así:

```{r, eval=FALSE}
llm_use("ollama", "llama3.2", seed = 100, temperature = 0)

# Otros modelos bajados: 

# llama3.2 
# deepseek-r1:1.5b (red neuronal con 1.5 mil millones de parámetros)
# deepseek-llm:7b (7 mil millones de parámetros -más pesada para la pc local 
# deepseek-r1:8b 
# llama3:8b 
# llama2:13b
```

el argumento seed permite resultados reproducibles, el argumento temperature controla el grado de creatividad del modelo de lenguaje. Se puede poner cualquier valor de 0 a 1, siendo 0 lo más lógico y determinista y 1 lo más creativo (a veces incoherente).

### Summarize

There may be a need to reduce the number of words in a given text. Typically to make it easier to understand its intent. The function has an argument to control the maximum number of words to output (max_words):

```{r}
dataset_llm |>
  llm_summarize(ab, max_words = 10)
```

La variable resumen no parece muy interesante para cruzar con otras existiendo la variable con el resumen completo. Igual no se pueden hacer cruces con tan pocos casos.

## Clasificación

```{r}
# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm_100 <- socio_ar_selec %>%
  slice_sample(n = 100)
nrow(dataset_llm_100)
```

```{r}
clasificador_1 <- dataset_llm_100 |>
  llm_classify(ab, c("subdisciplina sociológica 1 sobre desigualdad y estratificación (clase, género, raza)", "subdisciplina sociológica 2 sobre cultura y movimientos sociales", "subdisciplina sociológica 3 sobre sociología política/instituciones", "subdisciplina sociológica 4 sobre tecnología y sociedad (ej.: impacto de redes sociales",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí el artículo en alguna de las categorías propuestas, si no puedes crea una categoría residual llamada OTRA" ))
```
esto se contrapone en cierta forma al topic modeling y clustering, con la diferencia de que en este caso la clasificación sería supervisada, ya que las categorías están predefinidas. CLustering y Topic modeling no saben de antemano las categorías resultantes.

```{r}
clasificador_2 <- dataset_llm |>
  llm_classify(ab, 
      c("Desigualdad", 
      "Cultura", 
      "Política", 
      "Tecnología",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí cada artículo en alguna de las categorías propuestas, si no encuentras una adecuada crea una categoría llamada Otra " ))
```

## Sentiment analysis





