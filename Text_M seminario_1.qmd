---
title: "Código de seminario TextM 2025"
---

Se requieren los paquetes openalexR y tidyverse. Instalar con cuadros de diálogo y no con código.

```{r, message=FALSE, warning=FALSE}
library(openalexR)
library(tidyverse)
library(DataExplorer)
```

Explicar brevemente que es un base de datos bibiográfica y qué es Openalex y su comparación con tras bases de datos.

## Seleccionar lo que interesa

Utilizaremos como término de búsqueda las versiones correspondientes a los idiomas español, inglés, italiano, francés, portugues y alemán.

Explicar la busqueda:

```{r}
socio_ar <- oa_fetch(entity = "works",title.search = "sociologia OR sociology OR sociologie OR soziologie OR social",authorships.countries = "AR")
```

Visualizar las 39 variables que hay en el dataset:
```{r}
names(socio_ar)
```

seleccionar las que interesan
```{r}
socio_ar_selec <- socio_ar %>% select(id,title,ab,topics,publication_year)
```

### Análisis de valores perdidos

```{r}
socio_ar_selec %>%
  summarise(across(everything(), ~ mean(is.na(.)), .names = "prop_na_{col}"))
```

```{r, fig.width=5}
plot_missing(socio_ar_selec) #paquete DataExplorer
```

### Ver que artículos son
```{r}
socio_ar_selec %>%
  filter(if_any(everything(), is.na))
```

## Preprocessing

Primer paso del preprocesamiento: tokenización de los **títulos**:
```{r}
library(tidytext) #Instalar antes el paquete tidytext con desde cuadros de diálogo

palab_tit <- socio_ar_selec[, c("id","title")] %>%
  unnest_tokens(palabra,title)
```

Tokenización de los **resumenes**:
```{r}
palab_res <- socio_ar_selec[, c("id","ab")] %>%
  unnest_tokens(palabra,ab)
```

Unir los objetos de datos título y resumen
```{r}
palab_tit_res <- bind_rows(palab_tit, palab_res)
```

### Eliminación de palabras no significativas (stopwords)
```{r}
library(stopwords) #Instalar el paquete stopwords desde cuadros de diálogo
```

Antes vamos a obtener las stopwords ya elaboradas para diferentes idiomas:

```{r}
stopwords_es <- tibble(palabra = stopwords::stopwords("es"))
stopwords_en <- tibble(palabra = stopwords::stopwords("en"))
stopwords_it <- tibble(palabra = stopwords::stopwords("it"))
stopwords_fr <- tibble(palabra = stopwords::stopwords("fr"))
stopwords_pt <- tibble(palabra = stopwords::stopwords("pt"))
stopwords_de <- tibble(palabra = stopwords::stopwords("de"))
```

Eliminar palabras no significativas (stopwods) de la variable título y de título y resumen integrado

```{r}
palab_tit_limp <- palab_tit %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")
```

```{r}
palab_tit_res_limp <- palab_tit_res %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")  
```


## Análisis simples
### Frencuencia de palabras

```{r}
tit_freq <- palab_tit_limp %>%
  count(palabra, sort = TRUE)
tit_freq
```

```{r}
tit_res_freq <- palab_tit_res_limp %>%
  count(palabra, sort = TRUE)
tit_res_freq
```

## Analisis simples

### Tokenización de términos compuestos

En este caso bigramas

#Bigramas de los títulos ordenados por freq
```{r}
bigram_tit <- palab_tit_limp %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  select(bigram) %>%
  count(bigram, sort = TRUE)
bigram_tit
```

Bigramas de los títulos + resumen ordenados por freq
```{r}
bigram_tit_res <- palab_tit_res_limp %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  select(bigram) %>%
  count(bigram, sort = TRUE)
bigram_tit_res
```

## Nubes de palabras

Wordclouds

```{r}
library(wordcloud2) #Instalar antes el paquete Wordcloud2
```

Filtrar palabras con frecuencia mayor a 1 en el títulos:
```{r}
bigram_tit_f <- bigram_tit %>%
  filter(n > 1) 
```

Ahora en Título + resumen
```{r}
bigram_tit_res_f <- bigram_tit_res %>%
  filter(n > 1) 
```

Creación de la nube de palabras de los títulos:
```{r}
wordcloud2(bigram_tit_f, size = 0.7)
```

Ahora de títulos + resumen

```{r}
wordcloud2(bigram_tit_f, size = 0.7)
```

## Tests con LLMs

Experimento abajo como funciona el paquete Mall con este dataset de artículos socio_ar_selec.

```{r}
socio_ar_selec
```

Achico el dataset para las pruebas:
```{r}
# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm <- socio_ar_selec %>%
  slice_sample(n = 10)
```


### The Mall package

1. Instalar el paquete ollamar

```{r, message=FALSE, warning=FALSE}
library(ollamar)
```

2. Download and install Ollama from the official website:

https://ollama.com/download

3. Download an LLM model. For example, I have been developing this package using Llama 3.2 to test. To get that model you can run:

```{r}
ollamar::pull("llama3.2")
```
Los modelos se descargan una sola vez, como si fueran paquetes.

## Qué modelo usar:

```{r, eval=FALSE}
llm_use("ollama", "deepseek-r1", seed = 100, temperature = 0)
```

### Summarize

There may be a need to reduce the number of words in a given text. Typically to make it easier to understand its intent. The function has an argument to control the maximum number of words to output (max_words):

```{r}
dataset_llm |>
  llm_summarize(ab, max_words = 10)
```

## Clasificación

```{r}
llm_classify( 
  .data, 
  col, 
  labels, 
  pred_name = ".classify", 
  additional_prompt = "" 
)


dataset_llm |>
  llm_classify(ab, c("subdisciplina sociológica 1 sobre desigualdad y estratificación (clase, género, raza)", "subdisciplina sociológica 2 sobre cultura y movimientos sociales", "subdisciplina sociológica 3 sobre sociología política/instituciones", "subdisciplina sociológica 4 sobre tecnología y sociedad (ej.: impacto de redes sociales",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí el artículo en alguna de las categorías propuestas, si no puedes crea una categoría residual llamada OTRA" ))
```

```{r}
dataset_llm |>
  llm_classify(ab, 
      c("Desigualdad", 
      "Cultura", 
      "Política", 
      "Tecnología",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí cada artículo en alguna de las categorías propuestas, si no encuentras una adecuada crea una categoría llamada Otra " ))
```




