---
title: "Código de seminario TextM 2025"
---

Se requieren los paquetes openalexR, tidyverse, DataExplorer, tidytext, stopwords, Wordcloud2. Instalar con cuadros de diálogo y no con código.

```{r, message=FALSE, warning=FALSE}
library(openalexR)
library(tidyverse)
library(DataExplorer)
library(tidytext)
library(stopwords)
library(wordcloud2)
library(textclean)
```

# Selección de los datos a trabajar

## Seleccionar la fuente

En esta primera parte del seminario realizaremos las prácticas sobre un conjunto de datos textuales que emanan de **registros bibliográficos referenciales sobre sociología argentina**. El concepto de "referencial" es importante para entender que no estamos hablando de textos completos de artículos o libros, sino solo de sus datos bibliográficos: titulo, resumen, keywords, etc. Obedece a que resulta, en terminos prácticos, trabajar con un corpus textual acotado en cuanto a la longitud del texto de cada item, de esa manera se pueden realizar los checkeos de las acciones con mayor facilidad.

La elección de la temática de la producción argentina sobre temas sociológicos es totalmente arbitraria.  Podríamos haber aplicado cualquier otro recorte que desearamos. Lo que si resulta determinante, es que elegimos **OpenAlex** como fuente donde realizar la búsqueda.

OpenAlex (https://openalex.org) es una base de datos bibliográfica abierta, de alcance global, de reciente gestación (2022), pero con un crecimiento muy acelerado y que ofrece en la actualidad más de 250 millones de registros bibliográficos de producción científica de todo el mundo. Es multidiciplinar y pone su esfuerzo en incluir materiales en otros idiomas además del inglés. Tuvo su origen en Microsoft Academic Graph, producto que se discontinuó, y sirvió de base para iniciar este mega proyecto que cosecha más de 250 mil fuentes distintas. La base se puede consultar mediante una API REST pública, que es gratuita y no requiere autenticación. Sin embargo, tiene limitaciones de velocidad, con un límite de 100.000 llamadas por día y un máximo de 10 llamadas por segundo. Por fuera de eso, ofrece un servicio premium. En nuestro caso, interrogaremos la base de datos utilizando un paquete de R que fue especificamente diseñado para mediar en esta tarea, que se llama **openalexR**. Siemplemente nos exime del uso de la API de forma directa.  

La estructura de información que maneja esta base de datos tiene cierta complejidad y no es objetivo de este seminario abordarla. Simplemente, nos interesa reforzar la misma idea que ya se transmitió en los seminarios anteriores: que un buen conocimiento de la estructura de información subyacente de cualquier fuente, así como saber cuál es la forma correcta de interrogarla, son aspectos imprescindibles a conocer y que esto antecede a cualquier extracción de datos a los fines de ejecutar análisis serios.

## Buscar lo que interesa

Al trabajar con una fuente multilingue, una primera decisión es determinar los idiomas que nos interesan que estén representados en nuestro conjunto de producción científica. En este caso, utilizaremos como término de búsqueda las versiones correspondientes a los idiomas español, inglés, italiano, francés, portugués y alemán. Al incluir esos idiomas, consideramos que estamos recogiendo la mayoría de la producción de nuestros autores argentinos.

Otro aspecto importante es tener claro cuales son los campos de datos de la base de datos que se utilizan en la interrogación. Solo para dar ejemplos de variantes posibles, una cosa es buscar un *string* únicamente en los títulos, otro que aparezca en los resúmenes o en ambos. Recordemos que cada campo de una base de datos es una unidad lógica que puede ser explotada individualmente, o en conjunto con otras, si es que usamos operaciones lógicas Booleanas (AND, OR, NOT), por ejemplo. Sin embargo, estas cuestiones no siempre son tan transparentes en los productos y es necesario, como ya se dijo, conocer perfectamente la fuente por las implicancias que tiene a los fines de la recuperación de información.

En nuestro caso utilizaremos una forma de búsqueda muy sencilla, en la que indicamos que queremos obtener los registros bibliográficos de trabajos que contienen en el resumen "Sociología" o sus variantes en los idiomas antes indicados: sociología (ES), sociology (EN), sociologie (FR), sociologia (IT y PT) y soziologie (DE). Claramente está es una expresión de búsqueda muy rudimentaria, que está lejos de dar cuenta de la producción científica argentina sobre sociología, pero solo la tomamos como un ejemplo funcional a nuestra causa de colecta de datos a trabajar. 

El otro elemento que se incluye en la búsqueda, es la restricción de que los trabajos a recuperar deben tener al menos un autor que acompaña su firma con un lugar de trabajo en Argentina.

```{r}
socio_ar <- oa_fetch(abstract.search = "sociología OR sociologia OR sociology OR sociologie OR soziologie", authorships.institutions.country_code = "AR")
```

La función **oa_fetch** es propia del paquete **openalexR** y opera como si fuese una API de consulta a la base de datos, siguiendo los parámetros que se le indican como argumento. 

El resultado que arroja es un objeto R de tipo dataframe tibble, que de ahora en más pasa a constituir nuestro dataset de trabajo. Al mirarlo, comprobamos que hemos descargado 1895 trabajos (aprox. esto puede variar según el momento en el que s ehace la búsqueda y descarga, los cuales quedan descriptos mediante 44 variables: 

Visualizar las 44 variables que hay en el dataset:
```{r}
names(socio_ar)
```

Para nuestro ejercicio, vamos a trabajar con todas las observaciones pero no con todas las variables. Nos quedaremos solo con alguna básicas: el identificador del trabajo, el título, el resumen, los topicos y el año de publicación

seleccionar las que interesan
```{r}
socio_ar_selec <- socio_ar %>% 
  select(id,title,abstract,topics,publication_year)
```

## Análisis de valores perdidos

Una primera exploración sobre los datos que siempre es aconsejable hacer es el análisis de valores perdidos: 

```{r}
socio_ar_selec %>%
  summarise(across(everything(), ~ mean(is.na(.)), .names = "prop_na_{col}"))
```

Para visualir la proporción de faltantes de manera gráfica, se necesita el paquete DataExplorer. En este caso ya está instalado y corriendo.

```{r, fig.width=5}
plot_missing(socio_ar_selec) 
```

Para saber que trabajos son a los que le falta un dato, que de acuerdo a como lo muestra el gráfico anterior corresponde al título:

```{r}
socio_ar_selec %>%
  filter(if_any(everything(), is.na))
```

# PRE-PROCESAMIENTO

Una vez que contamos con el corpus textual a trabajar, comenzamos con algunos procesamientos que nos conducirán a obtener uno primeros resultados. El primero que proponemos es la generación de la NUBE DE PALABRAS que representa al corpus. Para ello se realizan algunos preprocesamientos como la *cleaning*, *tokenization*, *stopword* 

## Limpieza del texto

Principalmente se realiza una limpieza de los residuos que puedan quedar de HTML

socio_ar_selec$title <- socio_ar_selec$title %>%
  replace_html() %>%
  replace_contraction() %>%
  replace_non_ascii () %>%
  replace_url () %>%
  replace_email ()
  
socio_ar_selec$abstract <- socio_ar_selec$abstract %>%
  replace_html() %>%
  replace_contraction() %>%
  replace_non_ascii () %>%
  replace_url () %>%
  replace_email ()

## Tokenization 

Esto implica separar el texto en palabras. De las 5 variables que tiene nuestro corpus, solo 2 contienen textos ricos: los títulos y los resúmenes. Los tópicos, por el contrario son en muchos casos términos compuestos que no sería adecuado separar. Por ahora los excluímos.

```{r}
palab_tit <- socio_ar_selec[, c("id","title")] %>%
  unnest_tokens(palabra,title)
```
```{r}
palab_res <- socio_ar_selec[, c("id","ab")] %>%
  unnest_tokens(palabra,ab)
```
Generamos dos objetos de datos para hacer 2 nubes de palabras distintas. Una que corresponda solo a los títulos, y otra a los títulos y a los resúmenes. Por ello, previamente debemos tener otro objeto que las integre. Estos listados de palabras son dataframes tibble que contienen 2 variables: el id del documento y la palabra. Cómo veremos más adelante, si bien esto es lo que s edenomina una *bag of words*, es imprescindible mantener la información de a que item u observación pertenece cada una. 

```{r}
palab_tit_res <- bind_rows(palab_tit, palab_res)
```
## Stopword

Aplicar stopword es eliminar del listado general de palabras (o *bag of words*) aquellas que no tienen demasiada carga de sentido, tales como artículos, preposiciones, pronombres, etc. Para poder hacer esa tarea se debe saber cuales son esas palabras, y por ello, previamente, debemos contar con los listados en cada uno de los idiomas que están representados en el corpus:

```{r}
stopwords_es <- tibble(palabra = stopwords::stopwords("es"))
stopwords_en <- tibble(palabra = stopwords::stopwords("en"))
stopwords_it <- tibble(palabra = stopwords::stopwords("it"))
stopwords_fr <- tibble(palabra = stopwords::stopwords("fr"))
stopwords_pt <- tibble(palabra = stopwords::stopwords("pt"))
stopwords_de <- tibble(palabra = stopwords::stopwords("de"))
```
Una vez que tenemos los listados, podemos proceder a la eliminación. Lo haremos sobre el objeto que contiene las palabras de los títulos, por un lado, y en el objeto que tiene las de los títulos y resumenes, integradas. Esto lo hacemos para generar análisis comparativos.

```{r}
palab_tit_limp <- palab_tit %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")
```

```{r}
palab_tit_res_limp <- palab_tit_res %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")  
```

# PROCESAMIENTO 1: Análisis de frecuencia

Con los dos pasos anteriores -el de la tokenización y el de la eliminación de palabras no significativas-, hemos terminado de preparar la versión más simple de nuestro datos. El anmás simple es realizar un conteo de frecuencia de palabras como para ver si esa información nos dice algo sobre los perfiles temáticos de la producción científica argentina sobre Sociología. Vamos a incorporar en este análisis tambien la variable topics. Si bien provee menos cantidad de texto, como no la hemos tokenizado, mantiene la vinculación entre términos compuestos. 
La cantidad de observaciones que obtenemos nos está indicando la cantidad de palabras diferentes que contiene la variable(s) analizadas en la totalidad del corpus.

```{r}
tit_freq <- palab_tit_limp %>%
  count(palabra, sort = TRUE)
tit_freq
```

```{r}
tit_res_freq <- palab_tit_res_limp %>%
  count(palabra, sort = TRUE)
tit_res_freq
```

También vamos a aplicar este tipo de análisis sobre la variable topics(Tópicos). Si bien provee menos cantidad de texto que las variables título y resúmen, como no la hemos tokenizado, mantiene aún el vínculo entre los términos compuestos. Esta variable es un dataframe tibble en sí misma, es decir que estamos frente a un anidamiento de dataframes. Cuando la exploramos, vemos que los términos aparecen calificados con un "tipo". Los tipos se adjudican por especificidad creciente: dominio, campo, subcampo y tópico. Aquí nos interesa analisar los más específicos, es decir, aquellos que están calificados con el 4to nivel (tópic). Para ello generamos el objeto tópicos. Vemos que necesitamos escribir un poco más de código por esa condición de no ser una columna normal de un dataframe, sino de ser una columna que contiene a su vez un dataframe adentro (dataframe anidado): 

**unnest** permite desplegar el dataframe dentro del dataframe general
**mutate** sirve para renombrar las columnas
**filter** sirve para filtrar por la categoría de tópico. En este caso usaremos "topic"
**select** para seleccionar las columnas que me interesan 

topicos <- socio_ar_selec %>%
  unnest_longer (topics) %>%                                                                          
  mutate(topics_id = topics$id, topics_type = topics$type, topics_name = topics$display_name) %>%     
  filter(topics_type == "topic") %>%                                                                  
  select(topics_id,topics_name)                                                                       

Calculamos las frecuencias

top_freq <- topicos %>%
  count(palabra, sort = TRUE)
top_freq

En este punto, parece apropiado hacer un resumen que nos permita ver el tamaño de los distintos conjuntos d edatos que vamos obteniendo:

## Resumen
info <- c('Cant. de trabajos','Cant. palab títulos','Cant. palab títulos stopword','Cant. palab títulos únicas','Cant. palab títulos+resumen','Cant. palab títulos+resumen stopword','Cant. palab títulos+resumen únicas','Cant. tópicos','Cant. tópico únicos')
cant <- c(c(nrow(socio_ar),nrow(palab_tit),nrow(palab_tit_limp),nrow(tit_freq),nrow(palab_tit_res),nrow(palab_tit_res_limp),nrow(tit_res_freq),nrow(topicos),nrow(top_freq)))

resumen <- tibble(col1 = info,col2 = cant)
resumen

# PROCESAMIENTO 2: Tokenización de términos compuestos

Como hemos visto hasta aquí, trabajar con "bag of word" tiene el inconveniente de la desvinculación de las palabras que componen términos copuestos. Un tipo de procesamiento que se puede aplicar para tratar de encontrar una mejora en este sentido, es con la detección de bigramas, es decir con ventanas de dos palabras. También podrían ser tri-gramas, cuatri-gramas, y así. 

## Trabajar con bigramas

Obtenemos de los bigramas de los títulos ordenados por freq

```{r}
bigram_tit <- palab_tit_limp %>%
  select (id,palabra) %>%
  group_by(id) %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  count(id,bigram, sort = TRUE)
bigram_tit
```

Bigramas de los títulos + resumen ordenados por freq

```{r}
bigram_tit_res <- palab_tit_res_limp %>%
  select (id,palabra) %>%
  group_by(id) %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  count(bigram, sort = TRUE)
bigram_tit_res
```

Agregamos estos resultados al resumen:

resumen <- add_row(resumen,col1 = 'Cant. bigramas títulos con ID',col2 = nrow(bigram_tit_id))
resumen <- add_row(resumen,col1 = 'Cant. bigramas títulos+resumen con ID',col2 = nrow(bigram_tit_res_id))
resumen

# PROCESAMIENTO 3: Nubes de palabras

Antes de la generación de las nubes de palabras, aplicaremos, si lo creemos necesario, algún criterio de poda por frecuencia. En este caso, generaremos 3 nubes de palabras para poder comparar variantes. Generaremos la de los títulos por palabras individuales sin poda de frecuencias. La de los títulos por bigramas y la de tópicos las haremos con frecuencias mayores a 1.

# Título

bigram_tit_f <- bigram_tit_id %>%
  ungroup() %>%
  select(bigram,n) %>%
  filter(n > 1) 

# Tópicos
top_f <- top_freq %>%
  filter(n > 1) 

# Crear nube de palabras individuales de los títulos

nube_tit <- wordcloud2(tit_freq, size = 0.7)     
nube_tit

# Crear nube de bigramas de título

nube_tit_2 <- wordcloud2(bigram_tit_f, size = 0.2) 
nube_tit_2

# Tópicos
nube_top_2 <- wordcloud2(top_f, size = 0.6)
nube_top_2




***************************************************************************************************************************************************************************
## Grandes modelos de lenguaje 

Large Language Models (LLMs) de inteligencia artificial.

En este sección se usarán los paquetes ollamar, mall y ellmer.

* **Ollamar** es un paquete de R que actúa como una API de la plataforma Ollama. 
* **Ellmer**  es un paquete de R se utiliza para chatbots  funciona tanto con los modelos locales de Ollama como con otras APIs en la web como las de OpenIA, Mistral, DeepSeek, Anthropic, etc.
* **Mall** es un paquete de R se utiliza para datasets, y funciona con los modelos locales de Ollama.

## Ollamar

1.  Instalar el paquete ollamar

```{r, message=FALSE, warning=FALSE}
library(ollamar) #instalar antes en el rígido
```

2.  Bajar e instalar en la pc el soft de Ollama:

https://ollama.com/


3.  Bajar a la pc local algunos modelos para usar luego:

Navegar un poco la web de ollama buscando modelos. Estos se bajan al disco rígido sólo una vez.

```{r}
ollamar::pull("llama3.2") 
```

```{r}
ollamar::pull("deepseek-r1:1.5b") # red neuronal con 1.5 mil millones de parámetros!
```

```{r}
ollamar::pull("deepseek-llm:7b") # 7 mil millones de parámetros (más pesada para la pc local)
```

```{r}
ollamar::pull("deepseek-r1:8b")
```

```{r}
ollamar::pull("llama3:8b")
```

```{r}
ollamar::pull("llama2:13b")
```

## Ejercicios de aplicación 

## 1. Comunicación con chatbots de IA

### Ellmer Package

Este paquete solicitudes a cualquier proveedor de LLMs mediante la API de Ollama. También funciona con las APIs en la nube (ejemplo OpenAI, Mistral ect)

Ellmer usa LLMs locales:

* Ventajas: gratuidad, privacidad

* Desventajas: los LLMs son un poco menos avanzados que los disponibles vía web apis.

Empezaremos a usar los modelos locales provistos por Ollama. Esto tiene ventajas y desventajas como se explicó en la presentación de la clase.

### Chat básico con Ellmer

A diferencia de Rapp no voy a usar OpenIA sino Ollama. No necesito
setear clave con environmental variables.

Dee esta manera usamos con código modelos de lenguaje de la misma manera que lo hacemos con la interface de ChatGPT o cualquier otra:

```{r}
library(ellmer)  # Instalar previamente
```

```{r}
# Crea la sesión de chat y especifica el modelo local a usar
chat <- chat_ollama(    
  model = "llama3.2"
)
```

Ejecuto una consulta al modelo (prompt del usuario)
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Ahora creamos de nuevo la sesión, pero vamos a especificar el **system prompt**, parámetro que define instrucciones para guiar el comportamiento del modelo durante la sesión de chat:

```{r}
chat <- chat_ollama(
  model = "llama3.2",
  system_prompt = '"Sos un bot de AI que siempre responde lo siguiente: "Aguante Independiente de Avellaneda, el orgullo nacional!"'
)
```

Vuelvo a ejecutar la consulta:
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Observese como la respuesta cambia según el system_prompt:

```{r}
formal <- chat_ollama(model = "deepseek-llm:7b",  #cambio el modelo
                      system_prompt = "Actúa como un profesor universitario explicando conceptos de forma clara y formal.")

casual <- chat_ollama(model = "deepseek-llm:7b", 
                      system_prompt = "Habla como un amigo explicando de forma relajada y con ejemplos simples.")
```

```{r}
formal$chat("Explica en 70 palabras que es el overfitting")
```

```{r}
casual$chat("Explica en 100 palabras que es el overfitting")
```
También podemos pasar cualquier objeto externo, como una entrevista y pedirle al modelo que realice tareas de cualquier tipo, tal como hacemos con la interface habitual de ChatGPT. Aquí por ejemplo la vamos a pedir que resuma una entrevista. Se importa un entrevista como un objeto string (cadena de caracteres) y se asignan tareas al bot: generación de texto (analizar y resumir).

```{r}
#Bajar el archivo a la carpeta del proyecto desde este link: https://drive.google.com/file/d/135TC7Wy48UpYp-5V7wPmLF7btUdLSD3A/view?usp=sharing

# Leer el archivo de texto directamente como una sola cadena
entrevista <- paste(readLines("entrevista_militante.txt", 
                            encoding = "UTF-8", warn = FALSE), 
                   collapse = "\n") # une todas las línes leídas por readLines()
```


```{r}
modelo <- chat_ollama(model = "llama3.2",
                        system_prompt = "eres un bot muy conciso")

# 2. Crear el prompt completo
pregunta <- "Resume la entrevista en 100 palabras"
prompt_completo <- paste(pregunta, entrevista)  # Combina instrucción + texto input

# 3. Obtener y mostrar respuesta
resultado <- modelo$chat(prompt_completo)
resultado
```
La ventaja de usar chatbots mediante código es que permite integrarlos en procesos de datos.

## 2. Extracción de información de formatos no estructurados (IA extractiva)

Ellmer package. Se importa ahora un paper acádemico en formato como un objeto string (cadena de caracteres) y se asignan tareas:comprensión y extracción de información. Esto ya es más exigente, no para el modelo local sino para nuestras viejas PCs.



















```{r}
socio_ar_selec
```

Achico el dataset para las pruebas:
```{r}
set.seed(1234)  # Semilla aleatoria

# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm <- socio_ar_selec %>%
  sample_n(10)
```

Desenlistar la columna "topics", que es de tipo **list**: una lista en R es una estructura de datos versátil que puede contener elementos de diferentes tipos, como números, cadenas de texto, vectores e incluso otras listas. Una variable común en R almacena un único valor de un tipo de dato específico, mientras que una lista puede contener múltiples elementos de diferentes tipos (se puede entender como una mamushka)

```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest(topics, names_sep = "_") 
```

names_sep = "_" evita duplicaciones en nombres de columnas.

Ahora en cambio lo pivoteo para evitar la duplicación:
```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest_wider(topics, names_sep = "_")
```

### The Mall package

```{r}
library(mall) #se activa el paquete en la sesión
```

Al invocar las funciones de este paquete nos preguntará antes de correrla, que modelo de los bajados al rígido queremos usar. Por otro lado, también podemos elegir de antemano uno en particular así:

```{r, eval=FALSE}
llm_use("ollama", "llama3.2", seed = 100, temperature = 0)

# Otros modelos bajados: 

# llama3.2 
# deepseek-r1:1.5b (red neuronal con 1.5 mil millones de parámetros)
# deepseek-llm:7b (7 mil millones de parámetros -más pesada para la pc local 
# deepseek-r1:8b 
# llama3:8b 
# llama2:13b
```

el argumento seed permite resultados reproducibles, el argumento temperature controla el grado de creatividad del modelo de lenguaje. Se puede poner cualquier valor de 0 a 1, siendo 0 lo más lógico y determinista y 1 lo más creativo (a veces incoherente).

### Summarize

There may be a need to reduce the number of words in a given text. Typically to make it easier to understand its intent. The function has an argument to control the maximum number of words to output (max_words):

```{r}
dataset_llm |>
  llm_summarize(ab, max_words = 10)
```

La variable resumen no parece muy interesante para cruzar con otras existiendo la variable con el resumen completo. Igual no se pueden hacer cruces con tan pocos casos.

## Clasificación

```{r}
# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm_100 <- socio_ar_selec %>%
  slice_sample(n = 100)
nrow(dataset_llm_100)
```

```{r}
clasificador_1 <- dataset_llm_100 |>
  llm_classify(ab, c("subdisciplina sociológica 1 sobre desigualdad y estratificación (clase, género, raza)", "subdisciplina sociológica 2 sobre cultura y movimientos sociales", "subdisciplina sociológica 3 sobre sociología política/instituciones", "subdisciplina sociológica 4 sobre tecnología y sociedad (ej.: impacto de redes sociales",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí el artículo en alguna de las categorías propuestas, si no puedes crea una categoría residual llamada OTRA" ))
```
esto se contrapone en cierta forma al topic modeling y clustering, con la diferencia de que en este caso la clasificación sería supervisada, ya que las categorías están predefinidas. CLustering y Topic modeling no saben de antemano las categorías resultantes.

```{r}
clasificador_2 <- dataset_llm |>
  llm_classify(ab, 
      c("Desigualdad", 
      "Cultura", 
      "Política", 
      "Tecnología",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí cada artículo en alguna de las categorías propuestas, si no encuentras una adecuada crea una categoría llamada Otra " ))
```

## Sentiment analysis





