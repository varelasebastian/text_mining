---
title: "Código de seminario TextM 2025"
---

Se requieren los paquetes openalexR y tidyverse. Instalar con cuadros de diálogo y no con código.

```{r, message=FALSE, warning=FALSE}
library(openalexR)
library(tidyverse)
library(DataExplorer)
```

# Selección de los datos a trabajar

## Seleccionar la fuente

En esta primera parte del seminario realizaremos las prácticas sobre un conjunto de datos textuales que emanan de **registros bibliográficos referenciales de producción sobre sociología argentina**. El concepto de "referencial" aquí es importante para entender que no estamos hablando de textos completos de artículos o libros, sino solo de sus datos bibliográficos: titulo, resumen, keywords, etc. 

La elección de la temática de la producción argentina sobre temas sociológicos es totalmente arbitraria. Podríamos haber aplicado cualquier otro recorte que desearamos. Lo que si resulta determinante, es que elegimos **OpenAlex** como fuente donde realizar la búsqueda.

OpenAlex (https://openalex.org) es una base de datos bibliográfica abierta, de alcance global, de reciente gestación (2022), pero con un crecimiento muy acelerado y que ofrece en la actualidad más de 250 millones de registros bibliográficos de producción científica de todo el mundo. Es multidiciplinar y pone su esfuerzo en incluir materiales en otros idiomas, además del inglés. Tuvo su origen en Microsoft Academic Graph, producto que se discontinuó, y sirvió de base para iniciar este mega proyecto que cosecha más de 250 mil fuentes distintas.   

La estructura de información que maneja esta base de datos tiene cierta complejidad y no es objetivo de este seminario abordarla. Simplemente, nos interesa reforzar la misma idea que ya se transmitió en los seminarios anteriores: que un buen conocimiento de la estructura de información subyacente de cualquier fuente, así como saber cuál es la forma correcta de interrogarla, son aspectos imprescindibles conocer y que anteceden a cualquier extracción de datos.

## Buscar lo que interesa

Al trabajar con una fuente multilingue, una primera decisión es determinar los idiomas que nos interesan que estén representados en nuestro conjunto de producción científica. En este caso, utilizaremos como término de búsqueda las versiones correspondientes a los idiomas español, inglés, italiano, francés, portugués y alemán. Al incluir esos idiomas, consideramos que estamos recogiendo la mayoría de la producción de nuestros autores argentinos.

Otro aspecto importante es tener claro cuales son los campos de datos de la base de datos que se utilizan en la interrogación. Solo para dar ejemplos de variantes posibles, una cosa es buscar un *string* únicamente en los títulos, otro que aparezca en los resúmenes o en ambos. Recordemos que cada campo de una base de datos es una unidad lógica que puede ser explotada individualmente, o en conjunto con otras, si es que usamos operaciones lógicas Booleanas (AND, OR, NOT). Sin embargo, estas cuestiones no siempre son tan transparentes en los productos y es necesario, como ya se dijo, conocer perfectamente la fuente por las implicancias que tiene a los fines de la recuperación de información.

En nuestro caso utilizaremos una forma de búsqueda muy sencilla, en la que indicamos que queremos obtener los registros bibliográficos de trabajos que contienen en el título "Sociología". Así, luego de poner esta expresión bajo la lupa de los idiomas, determinamos que la expresión "sociolo" OR "soziolo", aplica un truncamiento válido para garantizar que se cubran todas las variantes idiomáticas necesarias: sociología (ES), sociology (EN), sociologie (FR), sociologia (IT y PT) y soziologie (DE). Claramente está es una expresión de búsqueda muy rudimentaria, que está lejos de dar cuenta de la producción científica argentina sobre sociología, pero solo tomemosla como un ejemplo funcional a nuestra causa de colecta de datos a trabajar. 

El otro elemento que se incluye en la búsqueda, es la restricción de que los trabajos a recuperar deben tener al menos un autor que acompaña su firma con un lugar de trabajo en Argentina.

```{r}
socio_ar <- oa_fetch(entity = "works",title.search = "sociolo OR soziolo",authorships.countries = "AR")
```

La función **oa_fetch** es propia del paquete **openalexR** y opera como si fuese una API de consulta a la base de datos, siguiendo los parámetros que s ele indican como argumento. 

El resultado que arroja es un objeto R de tipo ???, que de ahora en más pasa a constituir nuestro dataset de trabajo. Al mirarlo, comprobamos que hemos descargado ??? trabajos, los cuales quedan descriptos mediante 39 variables: 

Visualizar las 39 variables que hay en el dataset:
```{r}
names(socio_ar)
```

seleccionar las que interesan
```{r}
socio_ar_selec <- socio_ar %>% select(id,title,ab,topics,publication_year)
```

### Análisis de valores perdidos

```{r}
socio_ar_selec %>%
  summarise(across(everything(), ~ mean(is.na(.)), .names = "prop_na_{col}"))
```

```{r, fig.width=5}
plot_missing(socio_ar_selec) #paquete DataExplorer
```

Ver que artículos son
```{r}
socio_ar_selec %>%
  filter(if_any(everything(), is.na))
```

## Preprocessing

Primer paso del preprocesamiento: tokenización de los **títulos**:
```{r}
library(tidytext) #Instalar antes el paquete tidytext con desde cuadros de diálogo

palab_tit <- socio_ar_selec[, c("id","title")] %>%
  unnest_tokens(palabra,title)
```

Tokenización de los **resumenes**:
```{r}
palab_res <- socio_ar_selec[, c("id","ab")] %>%
  unnest_tokens(palabra,ab)
```

Unir los objetos de datos título y resumen
```{r}
palab_tit_res <- bind_rows(palab_tit, palab_res)
```

### Eliminación de palabras no significativas (stopwords)
```{r}
library(stopwords) #Instalar el paquete stopwords desde cuadros de diálogo
```

Antes vamos a obtener las stopwords ya elaboradas para diferentes idiomas:

```{r}
stopwords_es <- tibble(palabra = stopwords::stopwords("es"))
stopwords_en <- tibble(palabra = stopwords::stopwords("en"))
stopwords_it <- tibble(palabra = stopwords::stopwords("it"))
stopwords_fr <- tibble(palabra = stopwords::stopwords("fr"))
stopwords_pt <- tibble(palabra = stopwords::stopwords("pt"))
stopwords_de <- tibble(palabra = stopwords::stopwords("de"))
```

Eliminar palabras no significativas (stopwods) de la variable título y de título y resumen integrado

```{r}
palab_tit_limp <- palab_tit %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")
```

```{r}
palab_tit_res_limp <- palab_tit_res %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")  
```


## Análisis simples


### Frecuencia de palabras

```{r}
tit_freq <- palab_tit_limp %>%
  count(palabra, sort = TRUE)
tit_freq
```

```{r}
tit_res_freq <- palab_tit_res_limp %>%
  count(palabra, sort = TRUE)
tit_res_freq
```

## Analisis simples

### Tokenización de términos compuestos

En este caso bigramas

# Bigramas de los títulos ordenados por freq
```{r}
bigram_tit <- palab_tit_limp %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  select(bigram) %>%
  count(bigram, sort = TRUE)
bigram_tit
```

Bigramas de los títulos + resumen ordenados por freq
```{r}
bigram_tit_res <- palab_tit_res_limp %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  select(bigram) %>%
  count(bigram, sort = TRUE)
bigram_tit_res
```

## Nubes de palabras

Wordclouds

```{r}
library(wordcloud2) #Instalar antes el paquete Wordcloud2
```

Filtrar palabras con frecuencia mayor a 1 en el títulos:
```{r}
bigram_tit_f <- bigram_tit %>%
  filter(n > 1) 
```

Ahora en Título + resumen
```{r}
bigram_tit_res_f <- bigram_tit_res %>%
  filter(n > 1) 
```

Creación de la nube de palabras de los títulos:
```{r}
wordcloud2(bigram_tit_f, size = 0.7)
```

Ahora de títulos + resumen

```{r}
wordcloud2(bigram_tit_f, size = 0.7)
```

## Grandes modelos de lenguaje 

Large Language Models (LLMs) de inteligencia artificial.

En este sección se usarán los paquetes ollamar, mall y ellmer.

* **Ollamar** es un paquete de R que actúa como una API de la plataforma Ollama. 
* **Ellmer**  es un paquete de R se utiliza para chatbots  funciona tanto con los modelos locales de Ollama como con otras APIs en la web como las de OpenIA, Mistral, DeepSeek, Anthropic, etc.
* **Mall** es un paquete de R se utiliza para datasets, y funciona con los modelos locales de Ollama.

## Ollamar

1.  Instalar el paquete ollamar

```{r, message=FALSE, warning=FALSE}
library(ollamar) #instalar antes en el rígido
```

2.  Bajar e instalar en la pc el soft de Ollama:

https://ollama.com/


3.  Bajar a la pc local algunos modelos para usar luego:

Navegar un poco la web de ollama buscando modelos. Estos se bajan al disco rígido sólo una vez.

```{r}
ollamar::pull("llama3.2") 
```

```{r}
ollamar::pull("deepseek-r1:1.5b") # red neuronal con 1.5 mil millones de parámetros!
```

```{r}
ollamar::pull("deepseek-llm:7b") # 7 mil millones de parámetros (más pesada para la pc local)
```

```{r}
ollamar::pull("deepseek-r1:8b")
```

```{r}
ollamar::pull("llama3:8b")
```

```{r}
ollamar::pull("llama2:13b")
```

## Ejercicios de aplicación 

## 1. Comunicación con chatbots de IA

### Ellmer Package

Este paquete solicitudes a cualquier proveedor de LLMs mediante la API de Ollama. También funciona con las APIs en la nube (ejemplo OpenAI, Mistral ect)

Ellmer usa LLMs locales:

* Ventajas: gratuidad, privacidad

* Desventajas: los LLMs son un poco menos avanzados que los disponibles vía web apis.

Empezaremos a usar los modelos locales provistos por Ollama. Esto tiene ventajas y desventajas como se explicó en la presentación de la clase.

### Chat básico con Ellmer

A diferencia de Rapp no voy a usar OpenIA sino Ollama. No necesito
setear clave con environmental variables.

Dee esta manera usamos con código modelos de lenguaje de la misma manera que lo hacemos con la interface de ChatGPT o cualquier otra:

```{r}
library(ellmer)  # Instalar previamente
```

```{r}
# Crea la sesión de chat y especifica el modelo local a usar
chat <- chat_ollama(    
  model = "llama3.2"
)
```

Ejecuto una consulta al modelo (prompt del usuario)
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Ahora creamos de nuevo la sesión, pero vamos a especificar el **system prompt**, parámetro que define instrucciones para guiar el comportamiento del modelo durante la sesión de chat:

```{r}
chat <- chat_ollama(
  model = "llama3.2",
  system_prompt = '"Sos un bot de AI que siempre responde lo siguiente: "Aguante Independiente de Avellaneda, el orgullo nacional!"'
)
```

Vuelvo a ejecutar la consulta:
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Observese como la respuesta cambia según el system_prompt:

```{r}
formal <- chat_ollama(model = "deepseek-llm:7b",  #cambio el modelo
                      system_prompt = "Actúa como un profesor universitario explicando conceptos de forma clara y formal.")

casual <- chat_ollama(model = "deepseek-llm:7b", 
                      system_prompt = "Habla como un amigo explicando de forma relajada y con ejemplos simples.")
```

```{r}
formal$chat("Explica en 70 palabras que es el overfitting")
```

```{r}
casual$chat("Explica en 100 palabras que es el overfitting")
```
También podemos pasar cualquier objeto externo, como una entrevista y pedirle al modelo que realice tareas de cualquier tipo, tal como hacemos con la interface habitual de ChatGPT. Aquí por ejemplo la vamos a pedir que resuma una entrevista. Se importa un entrevista como un objeto string (cadena de caracteres) y se asignan tareas al bot: generación de texto (analizar y resumir).

```{r}
#Bajar el archivo a la carpeta del proyecto desde este link: https://drive.google.com/file/d/135TC7Wy48UpYp-5V7wPmLF7btUdLSD3A/view?usp=sharing

# Leer el archivo de texto directamente como una sola cadena
entrevista <- paste(readLines("entrevista_militante.txt", 
                            encoding = "UTF-8", warn = FALSE), 
                   collapse = "\n") # une todas las línes leídas por readLines()
```


```{r}
modelo <- chat_ollama(model = "llama3.2",
                        system_prompt = "eres un bot muy conciso")

# 2. Crear el prompt completo
pregunta <- "Resume la entrevista en 100 palabras"
prompt_completo <- paste(pregunta, entrevista)  # Combina instrucción + texto input

# 3. Obtener y mostrar respuesta
resultado <- modelo$chat(prompt_completo)
resultado
```
La ventaja de usar chatbots mediante código es que permite integrarlos en procesos de datos.

## 2. Extracción de información de formatos no estructurados (IA extractiva)

Ellmer package. Se importa ahora un paper acádemico en formato como un objeto string (cadena de caracteres) y se asignan tareas:comprensión y extracción de información. Esto ya es más exigente, no para el modelo local sino para nuestras viejas PCs.



















```{r}
socio_ar_selec
```

Achico el dataset para las pruebas:
```{r}
set.seed(1234)  # Semilla aleatoria

# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm <- socio_ar_selec %>%
  sample_n(10)
```

Desenlistar la columna "topics", que es de tipo **list**: una lista en R es una estructura de datos versátil que puede contener elementos de diferentes tipos, como números, cadenas de texto, vectores e incluso otras listas. Una variable común en R almacena un único valor de un tipo de dato específico, mientras que una lista puede contener múltiples elementos de diferentes tipos (se puede entender como una mamushka)

```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest(topics, names_sep = "_") 
```

names_sep = "_" evita duplicaciones en nombres de columnas.

Ahora en cambio lo pivoteo para evitar la duplicación:
```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest_wider(topics, names_sep = "_")
```

### The Mall package

```{r}
library(mall) #se activa el paquete en la sesión
```

Al invocar las funciones de este paquete nos preguntará antes de correrla, que modelo de los bajados al rígido queremos usar. Por otro lado, también podemos elegir de antemano uno en particular así:

```{r, eval=FALSE}
llm_use("ollama", "llama3.2", seed = 100, temperature = 0)

# Otros modelos bajados: 

# llama3.2 
# deepseek-r1:1.5b (red neuronal con 1.5 mil millones de parámetros)
# deepseek-llm:7b (7 mil millones de parámetros -más pesada para la pc local 
# deepseek-r1:8b 
# llama3:8b 
# llama2:13b
```

el argumento seed permite resultados reproducibles, el argumento temperature controla el grado de creatividad del modelo de lenguaje. Se puede poner cualquier valor de 0 a 1, siendo 0 lo más lógico y determinista y 1 lo más creativo (a veces incoherente).

### Summarize

There may be a need to reduce the number of words in a given text. Typically to make it easier to understand its intent. The function has an argument to control the maximum number of words to output (max_words):

```{r}
dataset_llm |>
  llm_summarize(ab, max_words = 10)
```

La variable resumen no parece muy interesante para cruzar con otras existiendo la variable con el resumen completo. Igual no se pueden hacer cruces con tan pocos casos.

## Clasificación

```{r}
# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm_100 <- socio_ar_selec %>%
  slice_sample(n = 100)
nrow(dataset_llm_100)
```

```{r}
clasificador_1 <- dataset_llm_100 |>
  llm_classify(ab, c("subdisciplina sociológica 1 sobre desigualdad y estratificación (clase, género, raza)", "subdisciplina sociológica 2 sobre cultura y movimientos sociales", "subdisciplina sociológica 3 sobre sociología política/instituciones", "subdisciplina sociológica 4 sobre tecnología y sociedad (ej.: impacto de redes sociales",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí el artículo en alguna de las categorías propuestas, si no puedes crea una categoría residual llamada OTRA" ))
```
esto se contrapone en cierta forma al topic modeling y clustering, con la diferencia de que en este caso la clasificación sería supervisada, ya que las categorías están predefinidas. CLustering y Topic modeling no saben de antemano las categorías resultantes.

```{r}
clasificador_2 <- dataset_llm |>
  llm_classify(ab, 
      c("Desigualdad", 
      "Cultura", 
      "Política", 
      "Tecnología",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí cada artículo en alguna de las categorías propuestas, si no encuentras una adecuada crea una categoría llamada Otra " ))
```

## Sentiment analysis





