---
title: "Código de seminario TextM 2025"
---

Se requieren los paquetes openalexR y tidyverse. Instalar con cuadros de diálogo y no con código.

```{r, message=FALSE, warning=FALSE}
library(openalexR)
library(tidyverse)
library(DataExplorer)
```

Explicar brevemente que es un base de datos bibiográfica y qué es Openalex y su comparación con tras bases de datos.

## Seleccionar lo que interesa

Utilizaremos como término de búsqueda las versiones correspondientes a los idiomas español, inglés, italiano, francés, portugues y alemán.

Explicar la busqueda:

```{r}
socio_ar <- oa_fetch(entity = "works",title.search = "sociologia OR sociology OR sociologie OR soziologie OR social",authorships.countries = "AR")
```

Visualizar las 39 variables que hay en el dataset:
```{r}
names(socio_ar)
```

seleccionar las que interesan
```{r}
socio_ar_selec <- socio_ar %>% select(id,title,ab,topics,publication_year)
```

### Análisis de valores perdidos

```{r}
socio_ar_selec %>%
  summarise(across(everything(), ~ mean(is.na(.)), .names = "prop_na_{col}"))
```

```{r, fig.width=5}
plot_missing(socio_ar_selec) #paquete DataExplorer
```

Ver que artículos son
```{r}
socio_ar_selec %>%
  filter(if_any(everything(), is.na))
```

## Preprocessing

Primer paso del preprocesamiento: tokenización de los **títulos**:
```{r}
library(tidytext) #Instalar antes el paquete tidytext con desde cuadros de diálogo

palab_tit <- socio_ar_selec[, c("id","title")] %>%
  unnest_tokens(palabra,title)
```

Tokenización de los **resumenes**:
```{r}
palab_res <- socio_ar_selec[, c("id","ab")] %>%
  unnest_tokens(palabra,ab)
```

Unir los objetos de datos título y resumen
```{r}
palab_tit_res <- bind_rows(palab_tit, palab_res)
```

### Eliminación de palabras no significativas (stopwords)
```{r}
library(stopwords) #Instalar el paquete stopwords desde cuadros de diálogo
```

Antes vamos a obtener las stopwords ya elaboradas para diferentes idiomas:

```{r}
stopwords_es <- tibble(palabra = stopwords::stopwords("es"))
stopwords_en <- tibble(palabra = stopwords::stopwords("en"))
stopwords_it <- tibble(palabra = stopwords::stopwords("it"))
stopwords_fr <- tibble(palabra = stopwords::stopwords("fr"))
stopwords_pt <- tibble(palabra = stopwords::stopwords("pt"))
stopwords_de <- tibble(palabra = stopwords::stopwords("de"))
```

Eliminar palabras no significativas (stopwods) de la variable título y de título y resumen integrado

```{r}
palab_tit_limp <- palab_tit %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")
```

```{r}
palab_tit_res_limp <- palab_tit_res %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") %>%
  anti_join(stopwords_fr, by = "palabra") %>%
  anti_join(stopwords_it, by = "palabra") %>%
  anti_join(stopwords_pt, by = "palabra") %>%
  anti_join(stopwords_de, by = "palabra")  
```


## Análisis simples


### Frecuencia de palabras

```{r}
tit_freq <- palab_tit_limp %>%
  count(palabra, sort = TRUE)
tit_freq
```

```{r}
tit_res_freq <- palab_tit_res_limp %>%
  count(palabra, sort = TRUE)
tit_res_freq
```

## Analisis simples

### Tokenización de términos compuestos

En este caso bigramas

# Bigramas de los títulos ordenados por freq
```{r}
bigram_tit <- palab_tit_limp %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  select(bigram) %>%
  count(bigram, sort = TRUE)
bigram_tit
```

Bigramas de los títulos + resumen ordenados por freq
```{r}
bigram_tit_res <- palab_tit_res_limp %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  select(bigram) %>%
  count(bigram, sort = TRUE)
bigram_tit_res
```

## Nubes de palabras

Wordclouds

```{r}
library(wordcloud2) #Instalar antes el paquete Wordcloud2
```

Filtrar palabras con frecuencia mayor a 1 en el títulos:
```{r}
bigram_tit_f <- bigram_tit %>%
  filter(n > 1) 
```

Ahora en Título + resumen
```{r}
bigram_tit_res_f <- bigram_tit_res %>%
  filter(n > 1) 
```

Creación de la nube de palabras de los títulos:
```{r}
wordcloud2(bigram_tit_f, size = 0.7)
```

Ahora de títulos + resumen

```{r}
wordcloud2(bigram_tit_f, size = 0.7)
```

## Grandes modelos de lenguaje 

Large Language Models (LLMs) de inteligencia artificial.

En este sección se usarán los paquetes ollamar, mall y ellmer.

* **Ollamar** es un paquete de R que actúa como una API de la plataforma Ollama. 
* **Mall** es un paquete de R se utiliza para datasets, y funciona con los modelos locales de Ollama.
* **Ellmer**  es un paquete de R se utiliza para chatbots  funciona tanto con los modelos locales de Ollama como con otras APIs en la web como las de OpenIA, Mistral, DeepSeek, Anthropic, etc.

## Ollamar

1.  Instalar el paquete ollamar

```{r, message=FALSE, warning=FALSE}
library(ollamar) #instalar antes en el rígido
```

2.  Bajar e instalar en la pc el soft de Ollama:

https://ollama.com/


3.  Bajar a la pc local algunos modelos para usar luego:

Navegar un poco la web de ollama buscando modelos. Estos se bajan al disco rígido sólo una vez.

```{r}
ollamar::pull("llama3.2") 
```

```{r}
ollamar::pull("deepseek-r1:1.5b") # red neuronal con 1.5 mil millones de parámetros!
```

```{r}
ollamar::pull("deepseek-llm:7b") # 7 mil millones de parámetros (más pesada para la pc local)
```

```{r}
ollamar::pull("deepseek-r1:8b")
```

```{r}
ollamar::pull("llama3:8b")
```

```{r}
ollamar::pull("llama2:13b")
```

## Ejercicios de aplicación 

## 1. Comunicación con chatbots




```{r}
socio_ar_selec
```

Achico el dataset para las pruebas:
```{r}
set.seed(1234)  # Semilla aleatoria

# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm <- socio_ar_selec %>%
  sample_n(10)
```

Desenlistar la columna "topics", que es de tipo **list**: una lista en R es una estructura de datos versátil que puede contener elementos de diferentes tipos, como números, cadenas de texto, vectores e incluso otras listas. Una variable común en R almacena un único valor de un tipo de dato específico, mientras que una lista puede contener múltiples elementos de diferentes tipos (se puede entender como una mamushka)

```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest(topics, names_sep = "_") 
```

names_sep = "_" evita duplicaciones en nombres de columnas.

Ahora en cambio lo pivoteo para evitar la duplicación:
```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest_wider(topics, names_sep = "_")
```

### The Mall package

```{r}
library(mall) #se activa el paquete en la sesión
```

Al invocar las funciones de este paquete nos preguntará antes de correrla, que modelo de los bajados al rígido queremos usar. Por otro lado, también podemos elegir de antemano uno en particular así:

```{r, eval=FALSE}
llm_use("ollama", "llama3.2", seed = 100, temperature = 0)

# Otros modelos bajados: 

# llama3.2 
# deepseek-r1:1.5b (red neuronal con 1.5 mil millones de parámetros)
# deepseek-llm:7b (7 mil millones de parámetros -más pesada para la pc local 
# deepseek-r1:8b 
# llama3:8b 
# llama2:13b
```

el argumento seed permite resultados reproducibles, el argumento temperature controla el grado de creatividad del modelo de lenguaje. Se puede poner cualquier valor de 0 a 1, siendo 0 lo más lógico y determinista y 1 lo más creativo (a veces incoherente).

### Summarize

There may be a need to reduce the number of words in a given text. Typically to make it easier to understand its intent. The function has an argument to control the maximum number of words to output (max_words):

```{r}
dataset_llm |>
  llm_summarize(ab, max_words = 10)
```

La variable resumen no parece muy interesante para cruzar con otras existiendo la variable con el resumen completo. Igual no se pueden hacer cruces con tan pocos casos.

## Clasificación

```{r}
# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm_100 <- socio_ar_selec %>%
  slice_sample(n = 100)
nrow(dataset_llm_100)
```

```{r}
clasificador_1 <- dataset_llm_100 |>
  llm_classify(ab, c("subdisciplina sociológica 1 sobre desigualdad y estratificación (clase, género, raza)", "subdisciplina sociológica 2 sobre cultura y movimientos sociales", "subdisciplina sociológica 3 sobre sociología política/instituciones", "subdisciplina sociológica 4 sobre tecnología y sociedad (ej.: impacto de redes sociales",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí el artículo en alguna de las categorías propuestas, si no puedes crea una categoría residual llamada OTRA" ))
```
esto se contrapone en cierta forma al topic modeling y clustering, con la diferencia de que en este caso la clasificación sería supervisada, ya que las categorías están predefinidas. CLustering y Topic modeling no saben de antemano las categorías resultantes.

```{r}
clasificador_2 <- dataset_llm |>
  llm_classify(ab, 
      c("Desigualdad", 
      "Cultura", 
      "Política", 
      "Tecnología",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí cada artículo en alguna de las categorías propuestas, si no encuentras una adecuada crea una categoría llamada Otra " ))
```

## Sentiment analysis





