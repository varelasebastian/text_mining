---
title: "Seminario Mineria de texto y modelos de lenguaje 2025"
toc: true
3toc-depth: 3
toc-title: "Tabla de contenidos"
author:
  - name: "Sebastián Varela"
    affiliation: "IdIHCs / Dpto de Sociología FaHCE UNLP"    
  - name: "Claudia González"
    affiliation: "IdIHCs / Dpto de Bibliotecología FaHCE UNLP" 
format: 
  pdf: 
    geometry: 
      - top=2cm
      - bottom=2cm
      - left=2cm
      - right=2cm
---

# **PARTE I:** MINERIA DE TEXTOS *(introductorio)*
**por Claudia M. González**

# Introducción

La minería de textos 
...

# Objetivo del módulo

En esta primera parte del seminario se revisarán algunas de las técnicas más tradicionales de la minería textual, así como los tratamiento previos más habituales a los que se someten los corpus textuales antes de ser analizados.

Se parte por conformar un conjunto de datos que se utilizará como caso de análisis en todo el módulo. De manera independiente a que los resultados obtenidos sean interesantes o no, su uso obedece a cuestiones meramente didácticas.

Durante los ejercicios se trabajará con diversos paquetes, los cuales deberán estar instalados previamente haciendo uso de los cuadros de diálogo de R Studio (opción Tools). A continuación se brinda una muy breve descripción de cada uno.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
```

**Tidyverse** es una colección de paquetes de R para la importación, transformación, manipulación y visualización de datos ordenados. 

```{r, message=FALSE, warning=FALSE}
library(openalexR)
```

**OpenalexR** es un paquete que permite recuperar información de la base de datos Open Alex y deja los datos preparados para ser manipulados en R base o por otros paquetes.

```{r, message=FALSE, warning=FALSE}
library(DataExplorer)
```

**DataExplorer** permite obtener salidas visuales útiles al realizar una primera exploración del conjunto de datos.

```{r, message=FALSE, warning=FALSE}
library(textclean)
```

**Textclean** permite detectar subcadenas cuya presencia no ayuda al momento del análisis. Puede borrarlas o normalizarlas.

```{r, message=FALSE, warning=FALSE}
library(tidytext)
```

**Tidytext** Entre sus principales funciones se encuentran las dedicadas a diferentes niveles de tokenización.

```{r, message=FALSE, warning=FALSE}
library(stopwords)
```

**Stopword** permite aplicar la limpieza de palabras no significativas en diferentes idiomas.

```{r, message=FALSE, warning=FALSE}
library(SnowballC)
```

**SnowballC** se utiliza para lematizar o llevar cada palabra a su raíz lingüistica 

```{r, message=FALSE, warning=FALSE}
library(wordcloud2)
library(webshot2)
library(htmlwidgets)
```

**wordcloud2** permite generar nubes de palabras. Los otros dos paquetes son necesarios para que las nubes se puedan imprimir en el PDF de salida.

```{r}
library(quanteda) 
library(quanteda.textstats)
library(quanteda.textplots)
```


# Selección de los datos a trabajar

## Seleccionar la fuente de datos

En esta primera parte del seminario realizaremos las prácticas sobre un conjunto de datos textuales que emanan de **registros bibliográficos referenciales sobre sociología argentina**. El concepto de *referencial* es importante para entender que no estamos hablando de textos completos de artículos o libros, sino solo de sus datos bibliográficos: titulo, resumen, keywords, etc. Obedece a que resulta, en terminos prácticos, trabajar con un corpus textual acotado en cuanto a la longitud del texto de cada item, de esa manera se pueden realizar los checkeos de las acciones con mayor facilidad.

La elección de la temática de la producción argentina sobre temas sociológicos es totalmente arbitraria. Podríamos haber aplicado cualquier otro recorte que desearamos. Lo que sí resulta determinante, es que elegimos **OpenAlex** como fuente donde realizar la búsqueda.

**OpenAlex** (https://openalex.org) es una base de datos bibliográfica abierta, de alcance global, de reciente gestación (2022), pero que ha tenido un crecimiento muy acelerado. Ofrece en la actualidad más de 250 millones de registros bibliográficos de producción científica de todo el mundo. Es multidiciplinar y pone su esfuerzo en incluir materiales en otros idiomas además del inglés. Tuvo su origen en Microsoft Academic Graph, producto que se discontinuó, y sirvió de base para iniciar este mega proyecto que cosecha más de 250 mil fuentes distintas. La base se puede consultar mediante una API REST pública, que es gratuita y no requiere autenticación. Sin embargo, tiene limitaciones de velocidad, con un límite de 100.000 llamadas por día y un máximo de 10 llamadas por segundo. Por fuera de eso, ofrece un servicio premium. En nuestro caso, interrogaremos la base de datos utilizando un paquete de R que fue especificamente diseñado para mediar en esta tarea, que se llama **openalexR**. Siemplemente nos exime del uso de la API de forma directa.

La estructura de información que maneja esta base de datos tiene cierta complejidad y no es objetivo de este seminario abordarla. Simplemente, nos interesa reforzar la misma idea que ya se transmitió en los seminarios anteriores: que un buen conocimiento de la estructura de información subyacente de cualquier fuente, así como saber cuál es la forma correcta de interrogarla, son aspectos imprescindibles a conocer y que esto antecede a cualquier extracción de datos a los fines de ejecutar análisis serios.

## Buscar lo que interesa

Al trabajar con una fuente multilingüe, una primera decisión es determinar los idiomas que nos interesan que estén representados en nuestro conjunto de producción científica. En este caso, utilizaremos como término de búsqueda las versiones correspondientes a los idiomas español, inglés, italiano, francés, portugués y alemán. Al incluir esos idiomas, consideramos que estamos recogiendo la mayoría de la producción de nuestros autores argentinos.

Otro aspecto importante es tener claro cuales son los campos de datos de la base de datos que se utilizan en la interrogación. Solo para dar ejemplos de variantes posibles, una cosa es buscar un *string* únicamente en los títulos, otro que aparezca en los resúmenes o en ambos. Recordemos que cada campo de una base de datos es una unidad lógica que puede ser explotada individualmente, o en conjunto con otras, si es que usamos operaciones lógicas Booleanas (AND, OR, NOT), por ejemplo. Sin embargo, estas cuestiones no siempre son tan transparentes en los productos y es necesario, como ya se dijo, conocer lo mejor que se pueda la fuente por las implicancias que tiene a los fines de la recuperación de información.

En nuestro caso utilizaremos una forma de búsqueda muy sencilla, en la que indicamos que queremos obtener los registros bibliográficos de trabajos que contienen en el resumen "Sociología" o sus variantes en los idiomas antes indicados: sociología (ES), sociology (EN), sociologie (FR), sociologia (IT y PT) y soziologie (DE). Claramente está es una expresión de búsqueda muy rudimentaria que está lejos de dar cuenta de la producción científica argentina sobre Sociología, pero solo la tomamos como un ejemplo funcional a nuestra causa de colecta de datos a trabajar.

El otro elemento que se incluye en la búsqueda, es la restricción de que los trabajos a recuperar deben tener al menos un autor que acompaña su firma con un lugar de trabajo en Argentina.

```{r}
socio_ar <- oa_fetch(abstract.search = "sociología OR sociologia OR sociology OR sociologie OR soziologie",
                     authorships.institutions.country_code = "AR")
```

La función **oa_fetch** es propia del paquete **openalexR** y opera como si fuese una API de consulta a la base de datos, siguiendo los parámetros que se le indican como argumento.

El resultado que arroja es un objeto R de tipo dataframe tibble, que de ahora en más pasa a constituir nuestro dataset de trabajo. Al mirarlo, comprobamos que hemos descargado 1895 trabajos los cuales están descriptos por 44 variables. La cantidad de trabajos puede variar según el momento en el que se hace la búsqueda y descarga.

Para visualizar las 44 variables que hay en el dataset:

```{r}
names(socio_ar)
```

En nuestro ejercicio, vamos a trabajar con todas las observaciones, pero no con todas las variables. Nos quedaremos solo con alguna básicas: el identificador del trabajo, el título, el resumen, los tópicos y el idioma del documento.

Para seleccionar las variables que interesan:

```{r}
socio_ar_selec <- socio_ar %>% 
  select(id,title,abstract,topics,language)
```

## Análisis de valores perdidos

Una primera exploración sobre los datos que siempre es aconsejable hacer es el análisis de valores perdidos. Para visualir la proporción de faltantes de manera gráfica, se necesita el paquete DataExplorer. En este caso ya está instalado y corriendo.

```{r, fig.width=5}
plot_missing(socio_ar_selec) 
```

Para saber que trabajos son a los que le falta un dato, que de acuerdo a como lo muestra el gráfico anterior corresponde al campo título e idioma:

```{r}
socio_ar_selec %>%
  filter(if_any(everything(), is.na))
```

Si se toma los últimos dígitos del ID de los trabajos, se puede buscar por cuadro de diálogo en el dataset para verificar que efectivamente son registros sin títulos o sin idioma.

# Pre-procesamiento de los textos

Una vez que contamos con el corpus textual a trabajar, comenzamos con algunos procesamientos que es necesario hacer antes de tratar de obtener unos primeros resultados. Para ello se realizan algunas tareas que permiten quitarle "ruido" al texto. Estas tareas son la **limpieza**, la ***tokenization***, la aplicación de ***stopword*** y el ***stemming***.

## Limpieza del texto

Principalmente se realiza una limpieza de los residuos que puedan quedar de HTML y de tildes:

```{r}
socio_ar_selec_limp$title <- socio_ar_selec$title %>%
  replace_html() %>%
  replace_contraction() %>%
  replace_non_ascii () %>%
  replace_url () %>%
  replace_email ()

  
socio_ar_selec_limp$abstract <- socio_ar_selec$abstract %>%
  replace_html() %>%
  replace_contraction() %>%
  replace_non_ascii () %>%
  replace_url () %>%
  replace_email ()
```

## Tokenization

Esto implica separar el texto en partes más pequeñas. Lo usual es hacerlo por palabras. Produce lo que se denomina una *bag of words* (BOW) o bolsa de palabras, que significa simplemente que se rompen las relaciones gramaticales que normalmente existen en el texto que integran. De las 6 variables que tiene nuestro corpus, solo 2 contienen textos ricos: los títulos y los resúmenes. Los tópicos, por el contrario son en muchos casos términos compuestos que no sería adecuado separar. Mientras título y resumen obedecen al lenguaje natural, los tópicos, por el contrario, obedecen al lenguaje controlado y eso tiene efectos diferentes como ya veremos. Por ahora excluímos los tópicos de la tokenización. Sin embargo, vamos a mantener la información de ID del documento y del idioma.

Para tokenizar por palabras:

```{r}
palab_tit <- socio_ar_selec[, c("id","title","language")] %>%
  unnest_tokens(palabra,title)

palab_res <- socio_ar_selec[, c("id","abstract","language")] %>%
  unnest_tokens(palabra,abstract)
```

Se generan dos objetos de datos para que luego se pueda comparar resultados. Uno corresponde a los títulos de los trabajos tokenizados, y el otro a los títulos + los resúmenes tokenizados. Estos listados de palabras son dataframes tibble que contienen 3 variables: el id del documento, la palabra y el idioma. Mantener la información del item u observación que aporta cada una de las palabras es lo que hace a la idea de **corpus**. Un **corpus** hace referencia a un conjunto de textos que durante ciertos tratamientos no deben perder su identidad, es decir que se pueden tratar como pequeñas BOW documento por documento. Al sumar título y resumen, consideramos que la repetición de términos en uno y otro campo servirá para reforzar de lo que trata el documento.

Para unir los títulos con los resúmenes:

```{r}
palab_tit_res <- bind_rows(palab_tit, palab_res)
```

## Stopword

Aplicar stopword es eliminar del listado general de palabras (o *bag of words*) aquellas que no tienen demasiada carga de sentido, tales como artículos, preposiciones, pronombres, etc. Para poder hacer esa tarea, se necesita contar previamente con listados de palabras a eliminar en cada idioma.

En este caso nos interesan los idiomas que están representados en nuestro corpus:

```{r}
stopwords_es <- tibble(palabra = stopwords::stopwords("es"),language = "es")
stopwords_en <- tibble(palabra = stopwords::stopwords("en"),language = "en")
stopwords_it <- tibble(palabra = stopwords::stopwords("it"),language = "it")
stopwords_fr <- tibble(palabra = stopwords::stopwords("fr"),language = "fr")
stopwords_pt <- tibble(palabra = stopwords::stopwords("pt"),language = "pt")
stopwords_de <- tibble(palabra = stopwords::stopwords("de"),language = "de")
```

Una vez que tenemos los listados, podemos proceder a la eliminación, la que simplemente se realiza por comparación de string de caracteres. Lo haremos sobre el conjunto de datos que contiene las palabras de los títulos, por un lado, y en el conjunto que tiene las palabras de los títulos + resúmenes. 

Para aplicar stopword usamos el anti_join, que sirve para filtrar las filas de un dataframe en función de que no estén presentes en otro. Lo hacemos por idiomas: 

```{r}
limp_tit_es <- palab_tit %>% 
  filter(language == "es") %>%
  anti_join(stopwords_es, by = "palabra") 

limp_tit_es 

limp_tit_res_es <- palab_tit_res %>% 
  filter(language == "es") %>%
  anti_join(stopwords_es, by = "palabra") 

limp_tit_en <- palab_tit %>% 
  filter(language == "en") %>%
  anti_join(stopwords_en, by = "palabra") 

limp_tit_res_en <- palab_tit_res %>% 
  filter(language == "en") %>%
  anti_join(stopwords_en, by = "palabra") 

limp_tit_fr <- palab_tit %>% 
  filter(language == "fr") %>%
  anti_join(stopwords_fr, by = "palabra") 

limp_tit_res_fr <- palab_tit_res %>% 
  filter(language == "fr") %>%
  anti_join(stopwords_fr, by = "palabra") 

limp_tit_it <- palab_tit %>% 
  filter(language == "it") %>%
  anti_join(stopwords_it, by = "palabra") 

limp_tit_res_it <- palab_tit_res %>% 
  filter(language == "it") %>%
  anti_join(stopwords_it, by = "palabra") 

limp_tit_pt <- palab_tit %>% 
  filter(language == "pt") %>%
  anti_join(stopwords_pt, by = "palabra") 

limp_tit_res_pt <- palab_tit_res %>% 
  filter(language == "pt") %>%
  anti_join(stopwords_pt, by = "palabra") 

limp_tit_de <- palab_tit %>% 
  filter(language == "de") %>%
  anti_join(stopwords_de, by = "palabra") 

limp_tit_res_de <- palab_tit_res %>% 
  filter(language == "de") %>%
  anti_join(stopwords_de, by = "palabra") 
```

* [caso a revisar 502678 título en español, abstract en inglés, language en o 610174 título y resumen en inglés, language es]

Juntamos los conjuntos de todos los idiomas en uno general. Lo hacemos tanto para título como para título + resumen:

```{r}
palab_tit_limp <- bind_rows(limp_tit_es,                        limp_tit_en,limp_tit_fr,limp_tit_it,limp_tit_pt)

palab_tit_res_limp <- bind_rows(limp_tit_res_es,
limp_tit_res_en,limp_tit_res_fr,limp_tit_res_it,limp_tit_res_pt)

palab_tit_limp <- palab_tit %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra") 
  
palab_tit_res_limp <- palab_tit_res %>%
  anti_join(stopwords_es, by = "palabra") %>%
  anti_join(stopwords_en, by = "palabra")  
```

## n-gramas (tokenización de términos compuestos)

Como hemos visto hasta aquí, trabajar con **bag of words** tiene el inconveniente de la desvinculación de las palabras que componen términos copuestos. Un tipo de procesamiento que se puede aplicar para tratar de encontrar una mejora en este sentido, es con la detección de bigramas, es decir con ventanas de dos palabras. También podrían ser tri-gramas, cuatri-gramas, y así.

a - Obtenemos los bigramas de los títulos ordenados por freq

```{r}
bigram_tit_id <- palab_tit_limp %>%
  select (id,palabra) %>%
  group_by(id) %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  count(id,bigram, sort = TRUE)
bigram_tit_id
```

b - Bigramas de los títulos + resumen ordenados por freq

```{r}
bigram_tit_res_id <- palab_tit_res_limp %>%
  select (id,palabra) %>%
  group_by(id) %>%
  mutate(next_word = lead(palabra)) %>%
  filter(!is.na(next_word)) %>%
  mutate(bigram = paste(palabra, next_word, sep = " ")) %>%
  count(bigram, sort = TRUE)
```

## Stemming

El *stemming* es el proceso de reducción de las variantes de las palabras hasta llegar a obtener el lema (entrada de diccionario). Los algoritmos que existen basan su procedimiento en la eliminación de prefijos o sufijo haciendo chequeos contra listas de comprobación y aplican algunas reglas propias por idioma. Se debe tener en cuenta que esto muchas veces hace que lo que se obtiene finalmente no sean palabras reales. Si bien el fin último es la lematización -llegar a entradas de diccionario-, no siempre es posible, y como es de esperar, los algoritmos trabajan mejor en el idioma inglés. El algoritmo más usado es el de Porter (1980), pero también son conocidos los llamados **S**, **Lovins** y **snowball**.

Aplicamos el stemming **snowball** de manera separada por idiomas:

```{r}
stem_tit_es <- palab_tit_limp %>%
  filter(language == "es") %>%
  mutate(stem = wordStem(palabra,language = "spanish"))

stem_tit_res_es <- palab_tit_res_limp %>%
  filter(language == "es") %>%
  mutate(stem = wordStem(palabra,language = "spanish"))

stem_tit_es  
```

```{r}
stem_tit_fr <- palab_tit_limp %>%
  filter(language == "fr") %>%
  mutate(stem = wordStem(palabra,language = "french"))

stem_tit_res_fr <- palab_tit_res_limp %>%
  filter(language == "fr") %>%
  mutate(stem = wordStem(palabra,language = "french"))

stem_tit_fr  
```

```{r}
stem_tit_it <- palab_tit_limp %>%
  filter(language == "it") %>%
  mutate(stem = wordStem(palabra,language = "italian"))

stem_tit_res_it <- palab_tit_res_limp %>%
  filter(language == "it") %>%
  mutate(stem = wordStem(palabra,language = "italian"))
```

```{r}
stem_tit_en <- palab_tit_limp %>%
  filter(language == "en") %>%
  mutate(stem = wordStem(palabra,language = "english"))

stem_tit_res_en <- palab_tit_res_limp %>%
  filter(language == "en") %>%
  mutate(stem = wordStem(palabra,language = "english"))
```

```{r}
stem_tit_pt <- palab_tit_limp %>%
  filter(language == "pt") %>%
  mutate(stem = wordStem(palabra,language = "portuguese"))

stem_tit_res_pt <- palab_tit_res_limp %>%
  filter(language == "pt") %>%
  mutate(stem = wordStem(palabra,language = "portuguese"))
```

Para el idioma alemán no lo hacemos, porque hemos verificado que en este corpus no existen trabajos en alemán.

Las volvemos a juntar en un solo dataframe

```{r}
palab_tit_stem <- bind_rows(stem_tit_es,stem_tit_en,stem_tit_fr,stem_tit_it,stem_tit_pt)
  
palab_tit_res_stem <- bind_rows(stem_tit_res_es,stem_tit_res_en,stem_tit_res_fr,stem_tit_res_it,stem_tit_res_pt)
```

# Técnicas

## Frecuencias y nubes de palabras

### Conteo de frecuencias
El conteo de frecuencias de palabras es el análisis más básico. La generación de NUBE DE PALABRAS que representa al corpus, es la expresión gráfica asociada.

Con los dos pasos anteriores -el de la tokenización y el de la eliminación de palabras no significativas-, hemos terminado de preparar la versión más simple de nuestro datos. Lo más simple es realizar un conteo de frecuencia de palabras como para ver si esa información nos dice algo sobre los perfiles temáticos de la producción científica argentina sobre Sociología. Vamos a incorporar en este análisis tambien la variable topics. Si bien provee menos cantidad de texto, como no la hemos tokenizado, mantiene la vinculación entre términos compuestos. La cantidad de observaciones que obtenemos nos está indicando la cantidad de palabras diferentes que contiene la variable(s) analizadas en la totalidad del corpus.

Para calcular la frecuencia lo hacemos sobre las palabras con stopword:

```{r}
tit_freq <- palab_tit_limp %>%
  count(palabra, sort = TRUE)
tit_freq
```

```{r}
tit_res_freq <- palab_tit_res_limp %>%
  count(palabra, sort = TRUE)
tit_res_freq
```

También vamos a aplicar este tipo de análisis sobre la variable topics(Tópicos). Si bien provee menos cantidad de texto que las variables título y resúmen, como no la hemos tokenizado, mantiene aún el vínculo entre los términos compuestos. Esta variable es un dataframe tibble en sí misma, es decir que estamos frente a un anidamiento de dataframes. Cuando la exploramos, vemos que los términos aparecen calificados con un "tipo". Los tipos se adjudican por especificidad creciente: dominio, campo, subcampo y tópico. Aquí nos interesa analisar los más específicos, es decir, aquellos que están calificados con el 4to nivel (tópic). Para ello generamos el objeto tópicos. Vemos que necesitamos escribir un poco más de código por esa condición de no ser una columna normal de un dataframe, sino de ser una columna que contiene a su vez un dataframe adentro (dataframe anidado):

**unnest** permite desplegar el dataframe dentro del dataframe general **mutate** sirve para renombrar las columnas **filter** sirve para filtrar por la categoría de tópico. En este caso usaremos "topic" **select** para seleccionar las columnas que me interesan

```{r}
topicos <- socio_ar_selec %>%
  unnest_longer (topics) %>%
  mutate(topics_id = topics$id, topics_type = topics$type,
         topics_name = topics$display_name) %>%
  filter(topics_type == "topic") %>%
  select(topics_id,topics_name)
```

Calculamos las frecuencias de los tópicos

```{r}
top_freq <- topicos %>%
  count(topics_name, sort = TRUE)
top_freq
```

### Obtener resumen 

En este punto, parece apropiado hacer un resumen que nos permita ver el tamaño de los distintos conjuntos de datos que vamos obteniendo:

```{r}
info <- c('Cant. de trabajos','Cant. palab títulos',
          'Cant. palab títulos stopword',
          'Cant. palab títulos únicas',
          'Cant. palab títulos+resumen',
          'Cant. palab títulos+resumen stopword',
          'Cant. palab títulos+resumen únicas',
          'Cant. tópicos','Cant. tópico únicos',
          'Cant. bigramas títulos con ID',
          'Cant. bigramas títulos+resumen con ID')
cant <- c(c(nrow(socio_ar),nrow(palab_tit),nrow(palab_tit_limp),
            nrow(tit_freq),nrow(palab_tit_res),
            nrow(palab_tit_res_limp),nrow(tit_res_freq),
            nrow(topicos),nrow(top_freq),nrow(bigram_tit_id),
            nrow(bigram_tit_res_id)))

resumen <- tibble(col1 = info,col2 = cant)
resumen
```

### Nubes de palabras

Antes de la generación de las nubes de palabras, aplicaremos, si lo creemos necesario, algún criterio de poda por frecuencia. En este caso, generaremos 3 nubes de palabras para poder comparar variantes. Generaremos la de los títulos por palabras individuales sin poda de frecuencias. La de los títulos por bigramas y la de tópicos las haremos con frecuencias mayores a 1.

```{r}
tit_freq_2 <- tit_freq %>%
  filter(n > 1) 
```

```{r}
bigram_tit_f <- bigram_tit_id %>%
  ungroup() %>%
  select(bigram,n) %>%
  filter(n > 1) 
```

```{r}
top_f <- top_freq %>%
  filter(n > 1) 
```

Nube de palabras de los títulos
```{r}
nube_tit <- wordcloud2(tit_freq, size = 0.8)
```

Esta parte de código que NO es importante, solo es necesaria para el renderizado y muestra de la nube en el PDF final.

```{r}
options(webshot.quiet = TRUE)
invisible({
html_temp <- tempfile(fileext = ".html")
  img_output <- "nube_tit.png"
  suppressMessages(saveWidget(nube_tit,html_temp,selfcontained = FALSE))
  capture.output(
    webshot2::webshot(html_temp, img_output, delay = 7, vwidth = 1000, vheight = 800,zoom = 2),
    file = nullfile())
  unlink(html_temp, force = TRUE)
  })
  
knitr::include_graphics(img_output)
```

Con un código similar, se pueden crear, por ejemplo, la de bigramas de títulos,

```{r}
nube_bigram <- wordcloud2(bigram_tit_f, size = 0.2)
```

Código renderizado PDF
```{r}
invisible({
html_temp <- tempfile(fileext = ".html")
  img_output <- "nube_bigram.png"
  suppressMessages(saveWidget(nube_bigram,html_temp,selfcontained = FALSE))
  capture.output(
    webshot2::webshot(html_temp, img_output, delay = 7, vwidth = 1000, vheight = 800,zoom = 2),
    file = nullfile())
  unlink(html_temp, force = TRUE)
})
  
knitr::include_graphics(img_output)
```

o la de tópicos
```{r}
nube_top <- wordcloud2(top_f, size = 0.6) 
```

Código renderizado PDF
```{r}
invisible({
html_temp <- tempfile(fileext = ".html")
  img_output <- "nube_top.png"
  suppressMessages(saveWidget(nube_top,html_temp,selfcontained = FALSE))
  capture.output(
    webshot2::webshot(html_temp, img_output, delay = 7, vwidth = 1000, vheight = 800,zoom = 2),
    file = nullfile())
  unlink(html_temp, force = TRUE)
})

knitr::include_graphics(img_output)
```

## TF-IDF y matrices término/documento

Ya se mostró que las nubes de palabras simplemente están mostrando las frecuencias contabilizadas a nivel del corpus completo, visto como una BOW. 

Ahora bien, siguiendo los antecedentes que propuso **Spark-Jones**, que a los fines de la recuperación de información, el mejor poder de discriminición se encuentra en las palabras que tienen una alta frecuencia intra-documental, pero también una baja frecuencia a nivel de la colección de documentos (corpus).

TF-IDF = TF * IDF

TF(t,d) = (Número de veces que aparece el término t en el documento d) / (Número total de términos en el documento d)

IDF(t) = log 10 ((Número total de documentos) / (Número de documentos que contienen el término t))

TF-IDF se ha utilizado ampliamente como una formula de ponderación de los términos en un corpus. 

La forma para realizar esos cálculos de frecuencias es organizar el corpus de manera bidimensional, donde las filas pueden ser todos los documentos y las columnas los términos de la totalidad del corpus (o a la inversa). Cada elemento individual, es decir la intersección de un término en un documento dado, puede tomar un valor simple como un 0 si el término está ausente o un 1 si está presente. Esto se conoce como **frecuencia binaria** aunque en rigor no es una frecuencia. Si en lugar de eso ponemos los valores de **frecuencias absolutas**, resultarán siempre favorecidos los documentos más largos (a mayor vocabulario má sposibilidad de repetición de términos). Otra forma para reducir la variabilidad que tenga el corpus en cuanto a la longitud de los documentos es utilizar lo que se denomina **frecuencia logarítmica** log 10 (1+frecuencia absoluta). La **frecuencia relativa** al tamaño del documento suele ser muy utilizada.Un poco más sofisticado sería, entonces, poner el valor resultante del cálculo TF-IDF. 

Un uso frecuente que se le da a estas matrices originarias de la RI (recuperación de información) es la TM (minería textual) con fines de determinar de que tratan los textos.

En esta práctica, se generan las matrices TF-IDF para el corpus de títulos:

```{r}
corpus_tit <- corpus(socio_ar_selec_limp$title)
tokens_tit <- tokens(corpus_tit,remove_punct = TRUE) %>%
  tokens_remove(stopwords("es")) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_remove(stopwords("fr")) %>%
  tokens_remove(stopwords("it")) %>%
  tokens_remove(stopwords("pt"))

dfm_tit <- dfm(tokens_tit)
dfm_tfidf_tit <- dfm_tfidf(dfm_tit)
```

Observamos que el resultado es una matriz gigante con más de 11 millones de elementos. Es rectangular de 1895 (documentos) x 5962 (términos o palabras). Las primeras 100 filas de la matriz muestran:

```{r}
top_terms <- textstat_frequency(dfm_tfidf_tit, n= 100, force =TRUE) %>%
  as_tibble()

top_terms
```

Esto se interpreta como: col1 = termino, col2 = suma global de los valores TF-IDF de ese termino en todos los documentos, col3 = rango = ranking (1 + frec), col4 = número de documentos donde aparece el término.

Si queremos podríamos eliminar los 3 primeros términos d ela ecuación diciendo que elimine del cálculo a los términos que estén en más de 150 documentos.

```{r}
dfm_tit_2 <- dfm_trim (
  dfm_tit,
  max_docfreq = 150)

dfm_tfidf_tit_2 <- dfm_tfidf(dfm_tit_2)
```

Técnicas posibles para visualizar estas matrices con algo de sentido: nuevamente las nubes de términos.

```{r}
quanteda.textplots::textplot_wordcloud(
  dfm_tfidf_tit_2,
  max_word = 60,
  color = c("blue", "red", "green"),
  rotation = 0.3,
  min_size = 0.5, 
  max_size = 4)
```

Se debe tener en cuenta que estas nubes no muestras nunca ni co-ocurrencia, ni correlación, ni distancias semánticas. Son solo frecuencias TF-IDF. Las palabras se disponen siguiendo un algoritmo de ubicación espacial óptima que gestiona los diferentes tamaños.

## Redes de co-palabras


# Bibliografía

Porter, M. F. 1980. “An Algorithm for Suffix Stripping.” Program 14 (3): 130–137. https://doi.org/10.1108/eb046814.


***************************************************************************************************************************************************************************
# PARTE II: Modelos de lenguaje *(introductorio)*

## Grandes modelos de lenguaje 

Large Language Models (LLMs) de inteligencia artificial.

En este sección se usarán los paquetes ollamar, mall y ellmer.

* **Ollamar** es un paquete de R que actúa como una API de la plataforma Ollama. 
* **Ellmer**  es un paquete de R se utiliza para chatbots  funciona tanto con los modelos locales de Ollama como con otras APIs en la web como las de OpenIA, Mistral, DeepSeek, Anthropic, etc.
* **Mall** es un paquete de R se utiliza para datasets, y funciona con los modelos locales de Ollama.

## Ollamar

1.  Instalar el paquete ollamar

```{r, message=FALSE, warning=FALSE}
library(ollamar) #instalar antes en el rígido
```

2.  Bajar e instalar en la pc el soft de Ollama:

https://ollama.com/


3.  Bajar a la pc local algunos modelos para usar luego:

Navegar un poco la web de ollama buscando modelos. Estos se bajan al disco rígido sólo una vez.

```{r}
ollamar::pull("llama3.2") 
```

```{r}
ollamar::pull("deepseek-r1:1.5b") # red neuronal con 1.5 mil millones de parámetros!
```

```{r}
ollamar::pull("deepseek-llm:7b") # 7 mil millones de parámetros (más pesada para la pc local)
```

```{r}
ollamar::pull("deepseek-r1:8b")
```

```{r}
ollamar::pull("llama3:8b")
```

```{r}
ollamar::pull("llama2:13b")
```

## Ejercicios de aplicación 

## 1. Comunicación con chatbots de IA

### Ellmer Package

Este paquete solicitudes a cualquier proveedor de LLMs mediante la API de Ollama. También funciona con las APIs en la nube (ejemplo OpenAI, Mistral ect)

Ellmer usa LLMs locales:

* Ventajas: gratuidad, privacidad

* Desventajas: los LLMs son un poco menos avanzados que los disponibles vía web apis.

Empezaremos a usar los modelos locales provistos por Ollama. Esto tiene ventajas y desventajas como se explicó en la presentación de la clase.

### Chat básico con Ellmer

A diferencia de Rapp no voy a usar OpenIA sino Ollama. No necesito
setear clave con environmental variables.

Dee esta manera usamos con código modelos de lenguaje de la misma manera que lo hacemos con la interface de ChatGPT o cualquier otra:

```{r}
library(ellmer)  # Instalar previamente
```

```{r}
# Crea la sesión de chat y especifica el modelo local a usar
chat <- chat_ollama(    
  model = "llama3.2"
)
```

Ejecuto una consulta al modelo (prompt del usuario)
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Ahora creamos de nuevo la sesión, pero vamos a especificar el **system prompt**, parámetro que define instrucciones para guiar el comportamiento del modelo durante la sesión de chat:

```{r}
chat <- chat_ollama(
  model = "llama3.2",
  system_prompt = '"Sos un bot de AI que siempre responde lo siguiente: "Aguante Independiente de Avellaneda, el orgullo nacional!"'
)
```

Vuelvo a ejecutar la consulta:
```{r}
chat$chat("Cuentame una broma sobre un científico de datos")
```
Observese como la respuesta cambia según el system_prompt:

```{r}
formal <- chat_ollama(model = "deepseek-llm:7b",  #cambio el modelo
                      system_prompt = "Actúa como un profesor universitario explicando conceptos de forma clara y formal.")

casual <- chat_ollama(model = "deepseek-llm:7b", 
                      system_prompt = "Habla como un amigo explicando de forma relajada y con ejemplos simples.")
```

```{r}
formal$chat("Explica en 70 palabras que es el overfitting")
```

```{r}
casual$chat("Explica en 100 palabras que es el overfitting")
```
También podemos pasar cualquier objeto externo, como una entrevista y pedirle al modelo que realice tareas de cualquier tipo, tal como hacemos con la interface habitual de ChatGPT. Aquí por ejemplo la vamos a pedir que resuma una entrevista. Se importa un entrevista como un objeto string (cadena de caracteres) y se asignan tareas al bot: generación de texto (analizar y resumir).

```{r}
#Bajar el archivo a la carpeta del proyecto desde este link: https://drive.google.com/file/d/135TC7Wy48UpYp-5V7wPmLF7btUdLSD3A/view?usp=sharing

# Leer el archivo de texto directamente como una sola cadena
entrevista <- paste(readLines("entrevista_militante.txt", 
                            encoding = "UTF-8", warn = FALSE), 
                   collapse = "\n") # une todas las línes leídas por readLines()
```


```{r}
modelo <- chat_ollama(model = "llama3.2",
                        system_prompt = "eres un bot muy conciso")

# 2. Crear el prompt completo
pregunta <- "Resume la entrevista en 100 palabras"
prompt_completo <- paste(pregunta, entrevista)  # Combina instrucción + texto input

# 3. Obtener y mostrar respuesta
resultado <- modelo$chat(prompt_completo)
resultado
```
La ventaja de usar chatbots mediante código es que permite integrarlos en procesos de datos.

## 2. Extracción de información de formatos no estructurados (IA extractiva)

Ellmer package. Se importa ahora un paper acádemico en formato como un objeto string (cadena de caracteres) y se asignan tareas:comprensión y extracción de información. Esto ya es más exigente, no para el modelo local sino para nuestras viejas PCs.



















```{r}
socio_ar_selec
```

Achico el dataset para las pruebas:
```{r}
set.seed(1234)  # Semilla aleatoria

# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm <- socio_ar_selec %>%
  sample_n(10)
```

Desenlistar la columna "topics", que es de tipo **list**: una lista en R es una estructura de datos versátil que puede contener elementos de diferentes tipos, como números, cadenas de texto, vectores e incluso otras listas. Una variable común en R almacena un único valor de un tipo de dato específico, mientras que una lista puede contener múltiples elementos de diferentes tipos (se puede entender como una mamushka)

```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest(topics, names_sep = "_") 
```

names_sep = "_" evita duplicaciones en nombres de columnas.

Ahora en cambio lo pivoteo para evitar la duplicación:
```{r}
dataset_llm_unlisted <- dataset_llm %>%
  unnest_wider(topics, names_sep = "_")
```

### The Mall package

```{r}
library(mall) #se activa el paquete en la sesión
```

Al invocar las funciones de este paquete nos preguntará antes de correrla, que modelo de los bajados al rígido queremos usar. Por otro lado, también podemos elegir de antemano uno en particular así:

```{r, eval=FALSE}
llm_use("ollama", "llama3.2", seed = 100, temperature = 0)

# Otros modelos bajados: 

# llama3.2 
# deepseek-r1:1.5b (red neuronal con 1.5 mil millones de parámetros)
# deepseek-llm:7b (7 mil millones de parámetros -más pesada para la pc local 
# deepseek-r1:8b 
# llama3:8b 
# llama2:13b
```

el argumento seed permite resultados reproducibles, el argumento temperature controla el grado de creatividad del modelo de lenguaje. Se puede poner cualquier valor de 0 a 1, siendo 0 lo más lógico y determinista y 1 lo más creativo (a veces incoherente).

### Summarize

There may be a need to reduce the number of words in a given text. Typically to make it easier to understand its intent. The function has an argument to control the maximum number of words to output (max_words):

```{r}
dataset_llm |>
  llm_summarize(ab, max_words = 10)
```

La variable resumen no parece muy interesante para cruzar con otras existiendo la variable con el resumen completo. Igual no se pueden hacer cruces con tan pocos casos.

## Clasificación

```{r}
# Extraer 30 filas aleatorias sin reemplazo (default)
dataset_llm_100 <- socio_ar_selec %>%
  slice_sample(n = 100)
nrow(dataset_llm_100)
```

```{r}
clasificador_1 <- dataset_llm_100 |>
  llm_classify(ab, c("subdisciplina sociológica 1 sobre desigualdad y estratificación (clase, género, raza)", "subdisciplina sociológica 2 sobre cultura y movimientos sociales", "subdisciplina sociológica 3 sobre sociología política/instituciones", "subdisciplina sociológica 4 sobre tecnología y sociedad (ej.: impacto de redes sociales",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí el artículo en alguna de las categorías propuestas, si no puedes crea una categoría residual llamada OTRA" ))
```
esto se contrapone en cierta forma al topic modeling y clustering, con la diferencia de que en este caso la clasificación sería supervisada, ya que las categorías están predefinidas. CLustering y Topic modeling no saben de antemano las categorías resultantes.

```{r}
clasificador_2 <- dataset_llm |>
  llm_classify(ab, 
      c("Desigualdad", 
      "Cultura", 
      "Política", 
      "Tecnología",
pred_name = "prod_type",
additional_prompt = "debes clasificar sí o sí cada artículo en alguna de las categorías propuestas, si no encuentras una adecuada crea una categoría llamada Otra " ))
```

## Sentiment analysis





